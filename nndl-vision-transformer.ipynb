{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-05-08T03:42:47.378845Z",
          "iopub.status.busy": "2025-05-08T03:42:47.378134Z",
          "iopub.status.idle": "2025-05-08T03:42:47.382851Z",
          "shell.execute_reply": "2025-05-08T03:42:47.382183Z",
          "shell.execute_reply.started": "2025-05-08T03:42:47.378820Z"
        },
        "trusted": true,
        "id": "nstcz6fNGq0_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "from typing import Dict, List, Optional, Text, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW72d6GUGq1A"
      },
      "source": [
        "# load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:42:47.384598Z",
          "iopub.status.busy": "2025-05-08T03:42:47.384211Z",
          "iopub.status.idle": "2025-05-08T03:42:47.399055Z",
          "shell.execute_reply": "2025-05-08T03:42:47.398486Z",
          "shell.execute_reply.started": "2025-05-08T03:42:47.384580Z"
        },
        "trusted": true,
        "id": "7NQKNV0cGq1B"
      },
      "outputs": [],
      "source": [
        "\"\"\"Constants for the data reader.\"\"\"\n",
        "\n",
        "INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph',\n",
        "                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
        "\n",
        "OUTPUT_FEATURES = ['FireMask', ]\n",
        "\n",
        "# Data statistics\n",
        "# For each variable, the statistics are ordered in the form:\n",
        "# (min_clip, max_clip, mean, standard deviation)\n",
        "DATA_STATS = {\n",
        "    # Elevation in m.\n",
        "    # 0.1 percentile, 99.9 percentile\n",
        "    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n",
        "    # Pressure\n",
        "    # 0.1 percentile, 99.9 percentile\n",
        "    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n",
        "    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),  # min, max\n",
        "    # Precipitation in mm.\n",
        "    # Negative values do not make sense, so min is set to 0.\n",
        "    # 0., 99.9 percentile\n",
        "    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n",
        "    # Specific humidity.\n",
        "    # Negative values do not make sense, so min is set to 0.\n",
        "    # The range of specific humidity is up to 100% so max is 1.\n",
        "    'sph': (0., 1., 0.0071658953, 0.0042835088),\n",
        "    # Wind direction in degrees clockwise from north.\n",
        "    # Thus min set to 0 and max set to 360.\n",
        "    'th': (0., 360.0, 190.32976, 72.59854),\n",
        "    # Min/max temperature in Kelvin.\n",
        "    # -20 degree C, 99.9 percentile\n",
        "    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n",
        "    # -20 degree C, 99.9 percentile\n",
        "    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n",
        "    # Wind speed in m/s.\n",
        "    # Negative values do not make sense, given there is a wind direction.\n",
        "    # 0., 99.9 percentile\n",
        "    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n",
        "    # NFDRS fire danger index energy release component expressed in BTU's per\n",
        "    # square foot.\n",
        "    # Negative values do not make sense. Thus min set to zero.\n",
        "    # 0., 99.9 percentile\n",
        "    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n",
        "    # Population density\n",
        "    # min, 99.9 percentile\n",
        "    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n",
        "    # We don't want to normalize the FireMasks.\n",
        "    # 1 indicates fire, 0 no fire, -1 unlabeled data\n",
        "    'PrevFireMask': (-1., 1., 0., 1.),\n",
        "    'FireMask': (-1., 1., 0., 1.)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:42:47.399868Z",
          "iopub.status.busy": "2025-05-08T03:42:47.399670Z",
          "iopub.status.idle": "2025-05-08T03:42:47.412637Z",
          "shell.execute_reply": "2025-05-08T03:42:47.411933Z",
          "shell.execute_reply.started": "2025-05-08T03:42:47.399854Z"
        },
        "trusted": true,
        "id": "kO0KKjUVGq1C"
      },
      "outputs": [],
      "source": [
        "\"\"\"Library of common functions used in deep learning neural networks.\n",
        "\"\"\"\n",
        "def random_crop_input_and_output_images(\n",
        "    input_img: tf.Tensor,\n",
        "    output_img: tf.Tensor,\n",
        "    sample_size: int,\n",
        "    num_in_channels: int,\n",
        "    num_out_channels: int,\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Randomly axis-align crop input and output image tensors.\n",
        "\n",
        "  Args:\n",
        "    input_img: tensor with dimensions HWC.\n",
        "    output_img: tensor with dimensions HWC.\n",
        "    sample_size: side length (square) to crop to.\n",
        "    num_in_channels: number of channels in input_img.\n",
        "    num_out_channels: number of channels in output_img.\n",
        "  Returns:\n",
        "    input_img: tensor with dimensions HWC.\n",
        "    output_img: tensor with dimensions HWC.\n",
        "  \"\"\"\n",
        "  combined = tf.concat([input_img, output_img], axis=2)\n",
        "  combined = tf.image.random_crop(\n",
        "      combined,\n",
        "      [sample_size, sample_size, num_in_channels + num_out_channels])\n",
        "  input_img = combined[:, :, 0:num_in_channels]\n",
        "  output_img = combined[:, :, -num_out_channels:]\n",
        "  return input_img, output_img\n",
        "\n",
        "\n",
        "def center_crop_input_and_output_images(\n",
        "    input_img: tf.Tensor,\n",
        "    output_img: tf.Tensor,\n",
        "    sample_size: int,\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Center crops input and output image tensors.\n",
        "\n",
        "  Args:\n",
        "    input_img: tensor with dimensions HWC.\n",
        "    output_img: tensor with dimensions HWC.\n",
        "    sample_size: side length (square) to crop to.\n",
        "  Returns:\n",
        "    input_img: tensor with dimensions HWC.\n",
        "    output_img: tensor with dimensions HWC.\n",
        "  \"\"\"\n",
        "  central_fraction = sample_size / input_img.shape[0]\n",
        "  input_img = tf.image.central_crop(input_img, central_fraction)\n",
        "  output_img = tf.image.central_crop(output_img, central_fraction)\n",
        "  return input_img, output_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:42:47.414210Z",
          "iopub.status.busy": "2025-05-08T03:42:47.414004Z",
          "iopub.status.idle": "2025-05-08T03:42:47.434671Z",
          "shell.execute_reply": "2025-05-08T03:42:47.434024Z",
          "shell.execute_reply.started": "2025-05-08T03:42:47.414189Z"
        },
        "trusted": true,
        "id": "bIp_TLf5Gq1D"
      },
      "outputs": [],
      "source": [
        "\"\"\"Dataset reader for Earth Engine data.\"\"\"\n",
        "\n",
        "def _get_base_key(key: Text) -> Text:\n",
        "  \"\"\"Extracts the base key from the provided key.\n",
        "\n",
        "  Earth Engine exports TFRecords containing each data variable with its\n",
        "  corresponding variable name. In the case of time sequences, the name of the\n",
        "  data variable is of the form 'variable_1', 'variable_2', ..., 'variable_n',\n",
        "  where 'variable' is the name of the variable, and n the number of elements\n",
        "  in the time sequence. Extracting the base key ensures that each step of the\n",
        "  time sequence goes through the same normalization steps.\n",
        "  The base key obeys the following naming pattern: '([a-zA-Z]+)'\n",
        "  For instance, for an input key 'variable_1', this function returns 'variable'.\n",
        "  For an input key 'variable', this function simply returns 'variable'.\n",
        "\n",
        "  Args:\n",
        "    key: Input key.\n",
        "\n",
        "  Returns:\n",
        "    The corresponding base key.\n",
        "\n",
        "  Raises:\n",
        "    ValueError when `key` does not match the expected pattern.\n",
        "  \"\"\"\n",
        "  match = re.match(r'([a-zA-Z]+)', key)\n",
        "  if match:\n",
        "    return match.group(1)\n",
        "  raise ValueError(\n",
        "      'The provided key does not match the expected pattern: {}'.format(key))\n",
        "\n",
        "\n",
        "def _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
        "  \"\"\"Clips and rescales inputs with the stats corresponding to `key`.\n",
        "\n",
        "  Args:\n",
        "    inputs: Inputs to clip and rescale.\n",
        "    key: Key describing the inputs.\n",
        "\n",
        "  Returns:\n",
        "    Clipped and rescaled input.\n",
        "\n",
        "  Raises:\n",
        "    ValueError if there are no data statistics available for `key`.\n",
        "  \"\"\"\n",
        "  base_key = _get_base_key(key)\n",
        "  if base_key not in DATA_STATS:\n",
        "    raise ValueError(\n",
        "        'No data statistics available for the requested key: {}.'.format(key))\n",
        "  min_val, max_val, _, _ = DATA_STATS[base_key]\n",
        "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
        "  return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n",
        "\n",
        "\n",
        "def _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
        "  \"\"\"Clips and normalizes inputs with the stats corresponding to `key`.\n",
        "\n",
        "  Args:\n",
        "    inputs: Inputs to clip and normalize.\n",
        "    key: Key describing the inputs.\n",
        "\n",
        "  Returns:\n",
        "    Clipped and normalized input.\n",
        "\n",
        "  Raises:\n",
        "    ValueError if there are no data statistics available for `key`.\n",
        "  \"\"\"\n",
        "  base_key = _get_base_key(key)\n",
        "  if base_key not in DATA_STATS:\n",
        "    raise ValueError(\n",
        "        'No data statistics available for the requested key: {}.'.format(key))\n",
        "  min_val, max_val, mean, std = DATA_STATS[base_key]\n",
        "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
        "  inputs = inputs - mean\n",
        "  return tf.math.divide_no_nan(inputs, std)\n",
        "\n",
        "def _get_features_dict(\n",
        "    sample_size: int,\n",
        "    features: List[Text],\n",
        ") -> Dict[Text, tf.io.FixedLenFeature]:\n",
        "  \"\"\"Creates a features dictionary for TensorFlow IO.\n",
        "\n",
        "  Args:\n",
        "    sample_size: Size of the input tiles (square).\n",
        "    features: List of feature names.\n",
        "\n",
        "  Returns:\n",
        "    A features dictionary for TensorFlow IO.\n",
        "  \"\"\"\n",
        "  sample_shape = [sample_size, sample_size]\n",
        "  features = set(features)\n",
        "  columns = [\n",
        "      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n",
        "      for _ in features\n",
        "  ]\n",
        "  return dict(zip(features, columns))\n",
        "\n",
        "\n",
        "def _parse_fn(\n",
        "    example_proto: tf.train.Example, data_size: int, sample_size: int,\n",
        "    num_in_channels: int, clip_and_normalize: bool,\n",
        "    clip_and_rescale: bool, random_crop: bool, center_crop: bool,\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Reads a serialized example.\n",
        "\n",
        "  Args:\n",
        "    example_proto: A TensorFlow example protobuf.\n",
        "    data_size: Size of tiles (square) as read from input files.\n",
        "    sample_size: Size the tiles (square) when input into the model.\n",
        "    num_in_channels: Number of input channels.\n",
        "    clip_and_normalize: True if the data should be clipped and normalized.\n",
        "    clip_and_rescale: True if the data should be clipped and rescaled.\n",
        "    random_crop: True if the data should be randomly cropped.\n",
        "    center_crop: True if the data should be cropped in the center.\n",
        "\n",
        "  Returns:\n",
        "    (input_img, output_img) tuple of inputs and outputs to the ML model.\n",
        "  \"\"\"\n",
        "  if (random_crop and center_crop):\n",
        "    raise ValueError('Cannot have both random_crop and center_crop be True')\n",
        "  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n",
        "  feature_names = input_features + output_features\n",
        "  features_dict = _get_features_dict(data_size, feature_names)\n",
        "  features = tf.io.parse_single_example(example_proto, features_dict)\n",
        "\n",
        "  if clip_and_normalize:\n",
        "    inputs_list = [\n",
        "        _clip_and_normalize(features.get(key), key) for key in input_features\n",
        "    ]\n",
        "  elif clip_and_rescale:\n",
        "    inputs_list = [\n",
        "        _clip_and_rescale(features.get(key), key) for key in input_features\n",
        "    ]\n",
        "  else:\n",
        "    inputs_list = [features.get(key) for key in input_features]\n",
        "\n",
        "  inputs_stacked = tf.stack(inputs_list, axis=0)\n",
        "  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n",
        "\n",
        "  outputs_list = [features.get(key) for key in output_features]\n",
        "  assert outputs_list, 'outputs_list should not be empty'\n",
        "  outputs_stacked = tf.stack(outputs_list, axis=0)\n",
        "\n",
        "  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n",
        "  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n",
        "                                            'but dimensions of outputs_stacked'\n",
        "                                            f' are {outputs_stacked_shape}')\n",
        "  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n",
        "\n",
        "  if random_crop:\n",
        "    input_img, output_img = random_crop_input_and_output_images(\n",
        "        input_img, output_img, sample_size, num_in_channels, 1)\n",
        "  if center_crop:\n",
        "    input_img, output_img = center_crop_input_and_output_images(\n",
        "        input_img, output_img, sample_size)\n",
        "  return input_img, output_img\n",
        "\n",
        "\n",
        "def get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n",
        "                batch_size: int, num_in_channels: int, compression_type: Text,\n",
        "                clip_and_normalize: bool, clip_and_rescale: bool,\n",
        "                random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n",
        "  \"\"\"Gets the dataset from the file pattern.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: Input file pattern.\n",
        "    data_size: Size of tiles (square) as read from input files.\n",
        "    sample_size: Size the tiles (square) when input into the model.\n",
        "    batch_size: Batch size.\n",
        "    num_in_channels: Number of input channels.\n",
        "    compression_type: Type of compression used for the input files.\n",
        "    clip_and_normalize: True if the data should be clipped and normalized, False\n",
        "      otherwise.\n",
        "    clip_and_rescale: True if the data should be clipped and rescaled, False\n",
        "      otherwise.\n",
        "    random_crop: True if the data should be randomly cropped.\n",
        "    center_crop: True if the data shoulde be cropped in the center.\n",
        "\n",
        "  Returns:\n",
        "    A TensorFlow dataset loaded from the input file pattern, with features\n",
        "    described in the constants, and with the shapes determined from the input\n",
        "    parameters to this function.\n",
        "  \"\"\"\n",
        "  if (clip_and_normalize and clip_and_rescale):\n",
        "    raise ValueError('Cannot have both normalize and rescale.')\n",
        "  dataset = tf.data.Dataset.list_files(file_pattern)\n",
        "  dataset = dataset.interleave(\n",
        "      lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.map(\n",
        "      lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n",
        "          x, data_size, sample_size, num_in_channels, clip_and_normalize,\n",
        "          clip_and_rescale, random_crop, center_crop),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:42:47.435483Z",
          "iopub.status.busy": "2025-05-08T03:42:47.435274Z",
          "iopub.status.idle": "2025-05-08T03:42:48.608478Z",
          "shell.execute_reply": "2025-05-08T03:42:48.607853Z",
          "shell.execute_reply.started": "2025-05-08T03:42:47.435441Z"
        },
        "trusted": true,
        "id": "h5sYQrssGq1E"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "SAMPLE_SIZE = 64\n",
        "\n",
        "train_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_train*',\n",
        "    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n",
        "    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n",
        "    clip_and_rescale=False, random_crop=True, center_crop=False)\n",
        "\n",
        "validation_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_eval*',\n",
        "    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n",
        "    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n",
        "    clip_and_rescale=False, random_crop=True, center_crop=False)\n",
        "\n",
        "test_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_test*',\n",
        "    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n",
        "    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n",
        "    clip_and_rescale=False, random_crop=True, center_crop=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For loading the dataset from google drive"
      ],
      "metadata": {
        "id": "cNCaryRXLJJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from google.colab import drive\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "SAMPLE_SIZE = 64\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Use glob to find all matching files\n",
        "train_files = glob.glob('/content/drive/MyDrive/wildfires/next_day_wildfire_spread_train*.tfrecord')\n",
        "validation_files = glob.glob('/content/drive/MyDrive/wildfires/next_day_wildfire_spread_eval*.tfrecord')\n",
        "test_files = glob.glob('/content/drive/MyDrive/wildfires/next_day_wildfire_spread_test*.tfrecord')\n",
        "\n",
        "# Create the datasets using the list of files\n",
        "train_dataset = get_dataset(train_files,\n",
        "    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n",
        "    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n",
        "    clip_and_rescale=False, random_crop=True, center_crop=False)\n",
        "\n",
        "validation_dataset = get_dataset(validation_files,\n",
        "    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n",
        "    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n",
        "    clip_and_rescale=False, random_crop=True, center_crop=False)\n",
        "\n",
        "test_dataset = get_dataset(test_files,\n",
        "    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n",
        "    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n",
        "    clip_and_rescale=False, random_crop=True, center_crop=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhcJ1Tx4KgTC",
        "outputId": "9e17e5b8-5c9b-493e-8a83-ecd74d592761"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:42:48.609525Z",
          "iopub.status.busy": "2025-05-08T03:42:48.609287Z",
          "iopub.status.idle": "2025-05-08T03:42:48.803212Z",
          "shell.execute_reply": "2025-05-08T03:42:48.802437Z",
          "shell.execute_reply.started": "2025-05-08T03:42:48.609507Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbtymGfSGq1E",
        "outputId": "f39aa7f6-8270-4be2-a92b-bdaedda9790d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 64, 64, 12) (32, 64, 64, 1)\n",
            "(32, 64, 64, 12) (32, 64, 64, 1)\n"
          ]
        }
      ],
      "source": [
        "for x, y in train_dataset.take(1):\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "for x, y in validation_dataset.take(1):\n",
        "    print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCGGbOfyGq1F"
      },
      "source": [
        "# convert to torch ( i dont know how to tensor flow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:42:48.804682Z",
          "iopub.status.busy": "2025-05-08T03:42:48.804485Z",
          "iopub.status.idle": "2025-05-08T03:42:48.810853Z",
          "shell.execute_reply": "2025-05-08T03:42:48.810043Z",
          "shell.execute_reply.started": "2025-05-08T03:42:48.804667Z"
        },
        "trusted": true,
        "id": "3TZPz899Gq1F"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "class TFToTorchDataset(Dataset):\n",
        "    def __init__(self, tf_dataset, clean=False):\n",
        "        self.samples = []\n",
        "        for x, y in tf_dataset.as_numpy_iterator():\n",
        "            for i in range(x.shape[0]):\n",
        "                # Convert x: (32, 32, 12) → (12, 32, 32)\n",
        "                x_i = tf.transpose(x[i], perm=[2, 0, 1]).numpy()\n",
        "                # Convert y: (32, 32, 1) → (1, 32, 32)\n",
        "                y_i = tf.transpose(y[i], perm=[2, 0, 1]).numpy()\n",
        "\n",
        "                if clean:\n",
        "                    if (y_i == -1).any():\n",
        "                        continue  # Skip this sample\n",
        "                self.samples.append((\n",
        "                    torch.tensor(x_i, dtype=torch.float32),\n",
        "                    torch.tensor(y_i, dtype=torch.float32)\n",
        "                ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:42:48.813267Z",
          "iopub.status.busy": "2025-05-08T03:42:48.813076Z",
          "iopub.status.idle": "2025-05-08T03:43:21.087737Z",
          "shell.execute_reply": "2025-05-08T03:43:21.087110Z",
          "shell.execute_reply.started": "2025-05-08T03:42:48.813253Z"
        },
        "trusted": true,
        "id": "dRnYCmj9Gq1F"
      },
      "outputs": [],
      "source": [
        "torch_dataset = TFToTorchDataset(train_dataset, clean=True)\n",
        "train_loader = torch.utils.data.DataLoader(torch_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "torch_dataset_val =  TFToTorchDataset(validation_dataset, clean=True)\n",
        "val_loader = torch.utils.data.DataLoader(torch_dataset_val, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:43:21.091532Z",
          "iopub.status.busy": "2025-05-08T03:43:21.091267Z",
          "iopub.status.idle": "2025-05-08T03:43:21.166586Z",
          "shell.execute_reply": "2025-05-08T03:43:21.165801Z",
          "shell.execute_reply.started": "2025-05-08T03:43:21.091514Z"
        },
        "trusted": true,
        "id": "e6HEdxDqGq1G"
      },
      "outputs": [],
      "source": [
        "N = 5\n",
        "dataiter = iter(train_loader)\n",
        "\n",
        "image_list = []\n",
        "label_list = []\n",
        "#assume batch size equal to 1, otherwise divide N by batch size\n",
        "for i in range(0, N):\n",
        "  image, label = next(dataiter)\n",
        "  image_list.append(image)\n",
        "  label_list.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:43:21.167691Z",
          "iopub.status.busy": "2025-05-08T03:43:21.167460Z",
          "iopub.status.idle": "2025-05-08T03:43:21.350637Z",
          "shell.execute_reply": "2025-05-08T03:43:21.349869Z",
          "shell.execute_reply.started": "2025-05-08T03:43:21.167674Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "JNvEtlxvGq1G",
        "outputId": "60990b13-f9f9-4857-e133-8bc6de332181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(len(label_list[0][0][0][0]))\n",
        "# print(label_list[0][0])\n",
        "plt.imshow(label_list[4][0][0], cmap='viridis', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9Ud8xSQGq1H"
      },
      "source": [
        "# fully conv model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:43:21.351988Z",
          "iopub.status.busy": "2025-05-08T03:43:21.351533Z",
          "iopub.status.idle": "2025-05-08T03:43:21.360260Z",
          "shell.execute_reply": "2025-05-08T03:43:21.359497Z",
          "shell.execute_reply.started": "2025-05-08T03:43:21.351962Z"
        },
        "trusted": true,
        "id": "9NBS2hUGGq1H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=12, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def conv_block(in_c, out_c):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "\n",
        "        self.encoder1 = conv_block(in_channels, 64)\n",
        "        self.encoder2 = conv_block(64, 128)\n",
        "        self.encoder3 = conv_block(128, 256)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        self.bottleneck = conv_block(256, 512)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.decoder3 = conv_block(512, 256)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.decoder2 = conv_block(256, 128)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.decoder1 = conv_block(128, 64)\n",
        "\n",
        "        self.output_layer = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(self.pool(e1))\n",
        "        e3 = self.encoder3(self.pool(e2))\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(self.pool(e3))\n",
        "\n",
        "        # Decoder\n",
        "        d3 = self.up3(b)\n",
        "        d3 = self.decoder3(torch.cat([d3, e3], dim=1))\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        d2 = self.decoder2(torch.cat([d2, e2], dim=1))\n",
        "\n",
        "        d1 = self.up1(d2)\n",
        "        d1 = self.decoder1(torch.cat([d1, e1], dim=1))\n",
        "\n",
        "        # return torch.sigmoid(self.output_layer(d1))\n",
        "        return self.output_layer(d1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:43:21.361101Z",
          "iopub.status.busy": "2025-05-08T03:43:21.360924Z",
          "iopub.status.idle": "2025-05-08T03:43:21.378996Z",
          "shell.execute_reply": "2025-05-08T03:43:21.378268Z",
          "shell.execute_reply.started": "2025-05-08T03:43:21.361079Z"
        },
        "trusted": true,
        "id": "SxO9tgYWGq1I"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_prediction(model, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_val, y_val = next(iter(val_loader))\n",
        "        x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "        y_val = (y_val == 1).float()\n",
        "\n",
        "        pred = model(x_val)\n",
        "        pred_bin = (pred > 0.5).float()\n",
        "\n",
        "        # Show first sample\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "        axs[0].imshow(x_val[0, 11].cpu(), cmap='gray')\n",
        "        axs[0].set_title(\"Previous Fire Mask\")\n",
        "        axs[1].imshow(y_val[0, 0].cpu(), cmap='gray')\n",
        "        axs[1].set_title(\"Ground Truth\")\n",
        "        axs[2].imshow(pred_bin[0, 0].cpu(), cmap='gray')\n",
        "        axs[2].set_title(\"Prediction\")\n",
        "        for ax in axs:\n",
        "            ax.axis('off')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:43:21.380040Z",
          "iopub.status.busy": "2025-05-08T03:43:21.379776Z",
          "iopub.status.idle": "2025-05-08T03:43:21.393922Z",
          "shell.execute_reply": "2025-05-08T03:43:21.393172Z",
          "shell.execute_reply.started": "2025-05-08T03:43:21.380014Z"
        },
        "trusted": true,
        "id": "5fvcnfGxGq1I"
      },
      "outputs": [],
      "source": [
        "def compute_iou(pred, target, threshold=0.5, eps=1e-6):\n",
        "    pred_bin = (pred > threshold).float()\n",
        "    target_bin = (target > 0.5).float()\n",
        "\n",
        "    intersection = (pred_bin * target_bin).sum(dim=(1, 2, 3))\n",
        "    union = (pred_bin + target_bin - pred_bin * target_bin).sum(dim=(1, 2, 3))\n",
        "    iou = (intersection + eps) / (union + eps)\n",
        "    return iou.mean().item()\n",
        "\n",
        "def compute_accuracy(pred, target, threshold=0.5):\n",
        "    pred_bin = (pred > threshold).float()\n",
        "    correct = (pred_bin == target).float()\n",
        "    return correct.mean().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:43:21.394843Z",
          "iopub.status.busy": "2025-05-08T03:43:21.394662Z",
          "iopub.status.idle": "2025-05-08T03:48:19.091634Z",
          "shell.execute_reply": "2025-05-08T03:48:19.090969Z",
          "shell.execute_reply.started": "2025-05-08T03:43:21.394830Z"
        },
        "trusted": true,
        "id": "9FuZKpIcGq1I"
      },
      "outputs": [],
      "source": [
        "def train_fcn_model(params):\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # model = UNet().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
        "\n",
        "    # Adjust pos_weight based on dataset statistics or params\n",
        "    weight_value = params.get(\"pos_weight\", 0.85 / 0.15)\n",
        "    pos_weight = torch.tensor([weight_value], device=device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    num_epochs = params.get(\"num_epochs\", 10)\n",
        "    batch_size = params.get(\"batch_size\", 32)\n",
        "\n",
        "    # Update train_loader and val_loader batch size if necessary\n",
        "    train_loader = torch.utils.data.DataLoader(torch_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(torch_dataset_val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * x.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # === Validation ===\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        total_iou = 0.0\n",
        "        total_acc = 0.0\n",
        "        n_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val in val_loader:\n",
        "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "                y_val = (y_val == 1).float()\n",
        "\n",
        "                pred_val = torch.sigmoid(model(x_val))\n",
        "                loss = criterion(pred_val, y_val)\n",
        "                val_loss += loss.item() * x_val.size(0)\n",
        "\n",
        "                # Metrics\n",
        "                batch_iou = compute_iou(pred_val, y_val)\n",
        "                batch_acc = compute_accuracy(pred_val, y_val)\n",
        "\n",
        "                total_iou += batch_iou * x_val.size(0)\n",
        "                total_acc += batch_acc * x_val.size(0)\n",
        "                n_samples += x_val.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        mean_iou = total_iou / n_samples\n",
        "        mean_acc = total_acc / n_samples\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d} | Train Loss: {train_loss:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | IoU: {mean_iou:.4f} | Acc: {mean_acc:.4f}\")\n",
        "\n",
        "    return val_loss  # Return validation loss for hyperparameter tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T03:48:39.739987Z",
          "iopub.status.busy": "2025-05-08T03:48:39.739283Z",
          "iopub.status.idle": "2025-05-08T03:48:39.924659Z",
          "shell.execute_reply": "2025-05-08T03:48:39.923909Z",
          "shell.execute_reply.started": "2025-05-08T03:48:39.739966Z"
        },
        "trusted": true,
        "id": "6AalOtO1Gq1J"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"lr\": 10 ** random.uniform(-5, -3)\n",
        "}\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet().to(device)\n",
        "train_fcn_model(params)\n",
        "show_prediction(model, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPsi-x9PGq1J"
      },
      "source": [
        "# vision transformer (IGNORE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T04:16:47.645740Z",
          "iopub.status.busy": "2025-05-08T04:16:47.645181Z",
          "iopub.status.idle": "2025-05-08T04:16:48.909794Z",
          "shell.execute_reply": "2025-05-08T04:16:48.908763Z",
          "shell.execute_reply.started": "2025-05-08T04:16:47.645718Z"
        },
        "trusted": true,
        "id": "Sm0HIw5BGq1K"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torchvision.models.vision_transformer import VisionTransformer\n",
        "\n",
        "# class FireMaskViT(nn.Module):\n",
        "#     def __init__(self, input_channels, image_size=64, patch_size=8, num_classes=1, hidden_dim=768, num_layers=12, num_heads=12):\n",
        "#         super().__init__()\n",
        "\n",
        "#         assert image_size % patch_size == 0, \"image_size must be divisible by patch_size\"\n",
        "\n",
        "#         num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "#         # VisionTransformer expects 3 input channels → we re-define input projection\n",
        "#         self.vit = VisionTransformer(\n",
        "#             image_size=image_size,\n",
        "#             patch_size=patch_size,\n",
        "#             num_layers=num_layers,\n",
        "#             num_heads=num_heads,\n",
        "#             hidden_dim=hidden_dim,\n",
        "#             mlp_dim=hidden_dim * 4,\n",
        "#             num_classes=0,  # no classification head\n",
        "#         )\n",
        "\n",
        "#         # Replace input projection layer to accept input_channels instead of 3\n",
        "#         self.vit.conv_proj = nn.Conv2d(input_channels, hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "#         # self.vit.encoder.pos_embedding = nn.Parameter(torch.randn(1, num_patches, hidden_dim))\n",
        "#         self.vit.encoder.cls_token = None\n",
        "#         self.vit.encoder.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, hidden_dim))\n",
        "\n",
        "\n",
        "\n",
        "#         # Output: project transformer embedding back to patch predictions\n",
        "#         # num_patches = (image_size // patch_size) ** 2\n",
        "#         self.output_head = nn.Linear(hidden_dim, patch_size * patch_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x: (batch_size, C, 64, 64)\n",
        "#         embeddings = self.vit(x)  # (batch_size, num_patches, hidden_dim)\n",
        "#         out = self.output_head(embeddings)  # (batch_size, num_patches, patch_pixels)\n",
        "#         # Reshape output back to (batch_size, 1, 64, 64)\n",
        "#         B, N, P = out.shape\n",
        "#         patch_size = int(P ** 0.5)\n",
        "#         H = W = int(N ** 0.5)\n",
        "#         out = out.view(B, H, W, patch_size, patch_size).permute(0,1,3,2,4).reshape(B, 1, H*patch_size, W*patch_size)\n",
        "#         return out\n",
        "\n",
        "# # Example usage\n",
        "# input_channels = 12  # prev fire mask + NDVI + temp + humidity + ...\n",
        "# model = FireMaskViT(input_channels=input_channels)\n",
        "\n",
        "# dummy_input = torch.randn(2, input_channels, 64, 64)\n",
        "# output = model(dummy_input)  # → output shape: (2, 1, 64, 64)\n",
        "# print(output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T04:15:25.087266Z",
          "iopub.status.busy": "2025-05-08T04:15:25.086572Z",
          "iopub.status.idle": "2025-05-08T04:15:26.227469Z",
          "shell.execute_reply": "2025-05-08T04:15:26.226456Z",
          "shell.execute_reply.started": "2025-05-08T04:15:25.087241Z"
        },
        "trusted": true,
        "id": "oL-74F8eGq1K"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torchvision.models.vision_transformer import VisionTransformer\n",
        "\n",
        "# class FireMaskViT(nn.Module):\n",
        "#     def __init__(self, input_channels=12, image_size=64, patch_size=8, hidden_dim=768, num_classes=1):\n",
        "#         super().__init__()\n",
        "\n",
        "#         assert image_size % patch_size == 0, \"image_size must be divisible by patch_size\"\n",
        "#         num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "#         # 1. Load pretrained ViT or define from scratch\n",
        "#         self.vit = VisionTransformer(\n",
        "#             image_size=image_size,\n",
        "#             patch_size=patch_size,\n",
        "#             num_layers=12,\n",
        "#             num_heads=12,\n",
        "#             hidden_dim=hidden_dim,\n",
        "#             mlp_dim=hidden_dim * 4,\n",
        "#             num_classes=0,  # No classifier head\n",
        "#         )\n",
        "\n",
        "#         # 2. Replace input stem to accept 12 channels\n",
        "#         self.vit.conv_proj = nn.Conv2d(input_channels, hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "#         # 3. Replace positional embedding with correct size\n",
        "#         self.vit.encoder.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, hidden_dim))\n",
        "\n",
        "#         # 4. Decoder head to produce patch_size x patch_size pixels per patch\n",
        "#         self.output_head = nn.Linear(hidden_dim, patch_size * patch_size)\n",
        "\n",
        "#         self.patch_size = patch_size\n",
        "#         self.image_size = image_size\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B = x.size(0)\n",
        "\n",
        "#         # x: (B, 12, 64, 64)\n",
        "#         x = self.vit(x)  # (B, num_patches, hidden_dim)\n",
        "#         x = self.output_head(x)  # (B, num_patches, patch_area)\n",
        "\n",
        "#         P = self.patch_size\n",
        "#         N = self.image_size // P\n",
        "#         out = x.view(B, N, N, P, P).permute(0, 1, 3, 2, 4).reshape(B, 1, self.image_size, self.image_size)\n",
        "#         return out\n",
        "\n",
        "\n",
        "# model = FireMaskViT(input_channels=12, image_size=64, patch_size=8)\n",
        "# dummy_input = torch.randn(2, 12, 64, 64)\n",
        "# out = model(dummy_input)\n",
        "# print(out.shape)  # Expected: torch.Size([2, 1, 64, 64])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gm5n5ViGq1L"
      },
      "source": [
        "# timm vt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T04:39:51.563178Z",
          "iopub.status.busy": "2025-05-08T04:39:51.562916Z",
          "iopub.status.idle": "2025-05-08T04:39:51.570047Z",
          "shell.execute_reply": "2025-05-08T04:39:51.569321Z",
          "shell.execute_reply.started": "2025-05-08T04:39:51.563159Z"
        },
        "trusted": true,
        "id": "t2VZfCX_Gq1L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "class FireMaskViT(nn.Module):\n",
        "    def __init__(self, input_channels=12, image_size=64, patch_size=8, hidden_dim=768):\n",
        "        super().__init__()\n",
        "\n",
        "        assert image_size % patch_size == 0, \"Image size must be divisible by patch size.\"\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        self.vit = timm.create_model(\n",
        "            'vit_base_patch16_224',\n",
        "            pretrained=False,\n",
        "            img_size=image_size,\n",
        "            patch_size=patch_size,\n",
        "            in_chans=input_channels,\n",
        "            num_classes=0  # no classifier head\n",
        "        )\n",
        "\n",
        "        # print(type(self.vit))\n",
        "\n",
        "\n",
        "        self.output_head = nn.Linear(self.vit.embed_dim, patch_size * patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        x = self.vit.forward_features(x)\n",
        "        x = x[:, 1:, :]\n",
        "        # print(\"vit output:\", x.shape)\n",
        "\n",
        "        x = self.output_head(x)  # shape (B, num_patches, patch_area)\n",
        "\n",
        "        P = self.patch_size\n",
        "        N = self.image_size // P  # patches along each dimension\n",
        "\n",
        "        x = x.view(B, N, N, P, P).permute(0, 1, 3, 2, 4).reshape(B, 1, self.image_size, self.image_size)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T04:38:15.842036Z",
          "iopub.status.busy": "2025-05-08T04:38:15.841758Z",
          "iopub.status.idle": "2025-05-08T04:38:19.231225Z",
          "shell.execute_reply": "2025-05-08T04:38:19.230502Z",
          "shell.execute_reply.started": "2025-05-08T04:38:15.842015Z"
        },
        "trusted": true,
        "id": "JSLIK1NxGq1L"
      },
      "outputs": [],
      "source": [
        "# model = FireMaskViT(input_channels=12, image_size=64, patch_size=8)\n",
        "# dummy_input = torch.randn(32, 12, 64, 64)\n",
        "# output = model(dummy_input)\n",
        "# print(output.shape)  # Should be: torch.Size([32, 1, 64, 64])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T04:51:36.379663Z",
          "iopub.status.busy": "2025-05-08T04:51:36.379105Z",
          "iopub.status.idle": "2025-05-08T04:51:37.626162Z",
          "shell.execute_reply": "2025-05-08T04:51:37.625581Z",
          "shell.execute_reply.started": "2025-05-08T04:51:36.379642Z"
        },
        "trusted": true,
        "id": "s9GZXHNZGq1M"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = FireMaskViT(input_channels=12).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "pos_weight = torch.tensor([10.0]).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# criterion = nn.BCEWithLogitsLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T04:51:39.503161Z",
          "iopub.status.busy": "2025-05-08T04:51:39.502542Z",
          "iopub.status.idle": "2025-05-08T04:59:41.420417Z",
          "shell.execute_reply": "2025-05-08T04:59:41.419578Z",
          "shell.execute_reply.started": "2025-05-08T04:51:39.503141Z"
        },
        "trusted": true,
        "id": "-m6rvCJfGq1M"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, 5):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_batch)\n",
        "        loss = criterion(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch:2d} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T05:00:21.536360Z",
          "iopub.status.busy": "2025-05-08T05:00:21.535688Z",
          "iopub.status.idle": "2025-05-08T05:00:21.541640Z",
          "shell.execute_reply": "2025-05-08T05:00:21.540837Z",
          "shell.execute_reply.started": "2025-05-08T05:00:21.536340Z"
        },
        "trusted": true,
        "id": "emKR6-e4Gq1M"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, threshold=0.5):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    intersection, union = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = torch.sigmoid(model(X)) > threshold\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += torch.numel(y)\n",
        "            intersection += (pred & (y > 0.5)).sum().item()\n",
        "            union += ((pred | (y > 0.5))).sum().item()\n",
        "    acc = correct / total\n",
        "    iou = intersection / union if union else 0\n",
        "    print(f\"Pixel Accuracy: {acc:.4f}, IoU: {iou:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T04:59:52.118467Z",
          "iopub.status.busy": "2025-05-08T04:59:52.117806Z",
          "iopub.status.idle": "2025-05-08T04:59:53.030251Z",
          "shell.execute_reply": "2025-05-08T04:59:53.029593Z",
          "shell.execute_reply.started": "2025-05-08T04:59:52.118429Z"
        },
        "trusted": true,
        "id": "a-nAjPwDGq1M"
      },
      "outputs": [],
      "source": [
        "show_prediction(model, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T05:00:34.209196Z",
          "iopub.status.busy": "2025-05-08T05:00:34.208675Z",
          "iopub.status.idle": "2025-05-08T05:00:39.030196Z",
          "shell.execute_reply": "2025-05-08T05:00:39.029428Z",
          "shell.execute_reply.started": "2025-05-08T05:00:34.209172Z"
        },
        "trusted": true,
        "id": "pme1XWekGq1N"
      },
      "outputs": [],
      "source": [
        "evaluate(model, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlaGbfwEGq1N"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pchrLaasGq1N"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataset, val_dataset, params, device):\n",
        "    \"\"\"\n",
        "    A reusable training loop that dynamically creates components based on hyperparameter tuning parameters.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to train (e.g., U-Net or Vision Transformer).\n",
        "        train_dataset (Dataset): The training dataset.\n",
        "        val_dataset (Dataset): The validation dataset.\n",
        "        params (dict): Dictionary containing hyperparameters (e.g., learning rate, batch size, optimizer type).\n",
        "        device (torch.device): Device to train on (e.g., 'cuda' or 'cpu').\n",
        "        num_epochs (int): Number of epochs to train for.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing training and validation losses, IoU, and accuracy for each epoch.\n",
        "    \"\"\"\n",
        "    # === Create DataLoaders ===\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=params[\"batch_size\"], shuffle=True\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=params[\"batch_size\"], shuffle=False\n",
        "    )\n",
        "\n",
        "    # === Define Optimizer ===\n",
        "    if params[\"optimizer\"] == \"adam\":\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n",
        "    elif params[\"optimizer\"] == \"adamw\":\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n",
        "    elif params[\"optimizer\"] == \"sgd\":\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"], momentum=0.9)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {params['optimizer']}\")\n",
        "\n",
        "    num_epochs = params.get(\"num_epochs\", 10)\n",
        "\n",
        "    # === Define Loss Function ===\n",
        "    pos_weight = params.get(\"pos_weight\", 0.85 / 0.15) # Default to 1.0 if not provided\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
        "\n",
        "    # === Training Loop ===\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_iou\": [],\n",
        "        \"val_acc\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # === Training ===\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(x_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "\n",
        "        # === Validation ===\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        total_iou = 0.0\n",
        "        total_acc = 0.0\n",
        "        n_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val in val_loader:\n",
        "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(x_val)\n",
        "                loss = criterion(outputs, y_val)\n",
        "                val_loss += loss.item() * x_val.size(0)\n",
        "\n",
        "                # Metrics\n",
        "                batch_iou = compute_iou(torch.sigmoid(outputs), y_val)\n",
        "                batch_acc = compute_accuracy(torch.sigmoid(outputs), y_val)\n",
        "\n",
        "                total_iou += batch_iou * x_val.size(0)\n",
        "                total_acc += batch_acc * x_val.size(0)\n",
        "                n_samples += x_val.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        mean_iou = total_iou / n_samples\n",
        "        mean_acc = total_acc / n_samples\n",
        "\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_iou\"].append(mean_iou)\n",
        "        history[\"val_acc\"].append(mean_acc)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | \"\n",
        "              f\"IoU: {mean_iou:.4f} | Acc: {mean_acc:.4f}\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ4gxdLLGq1N"
      },
      "source": [
        "# Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cnTYC53FGq1O"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from itertools import count\n",
        "\n",
        "def random_search(model_type, device, n_trials=3):\n",
        "    best_val_loss = float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    for trial in range(n_trials):\n",
        "        print(\"trial number \", trial+1)\n",
        "        if model_type == \"unet\":\n",
        "            params = {\n",
        "                \"lr\": 10 ** random.uniform(-5, -3),\n",
        "                \"batch_size\": random.choice([16, 32, 64]),\n",
        "                \"optimizer\": random.choice([\"adam\", \"adamw\", \"sgd\"]),\n",
        "                \"weight_decay\": random.choice([0, 1e-4, 1e-3]),\n",
        "                \"num_epochs\": 3,\n",
        "            }\n",
        "            # val_loss = train_fcn_model(params)\n",
        "            model = UNet().to(device)\n",
        "            # model.summary()\n",
        "        elif model_type == \"vit\":\n",
        "            patch_size = random.choice([8, 16, 32])\n",
        "            params = {\n",
        "                \"lr\": 10 ** random.uniform(-5, -4),\n",
        "                \"batch_size\": random.choice([16, 32, 64]),\n",
        "                \"patch_size\": patch_size,\n",
        "                \"n_heads\": random.choice([4, 8, 12]),\n",
        "                \"optimizer\": random.choice([\"adam\", \"adamw\", \"sgd\"]),\n",
        "                \"weight_decay\": random.uniform(1e-6, 1e-3),\n",
        "                \"num_epochs\": 3,\n",
        "            }\n",
        "            model = FireMaskViT(input_channels=12, image_size=64, patch_size=patch_size).to(device)\n",
        "        history = train_model(model, torch_dataset, torch_dataset_val, params, device)\n",
        "\n",
        "\n",
        "        val_loss = history[\"val_loss\"][-1]\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_params = params\n",
        "            # best_model = model\n",
        "\n",
        "    return best_params#, best_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "best_params_unet = random_search(model_type = \"unet\", device = device)\n",
        "print(best_params_unet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1nE853BMBlZ",
        "outputId": "915900bb-6cbd-4e87-808c-e5541d4ddc6f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "trial number  1\n",
            "Epoch 1/3 | Train Loss: 0.3081 | Val Loss: 0.2513 | IoU: 0.2056 | Acc: 0.9674\n",
            "Epoch 2/3 | Train Loss: 0.1767 | Val Loss: 0.1973 | IoU: 0.2269 | Acc: 0.9732\n",
            "Epoch 3/3 | Train Loss: 0.1354 | Val Loss: 0.1904 | IoU: 0.2217 | Acc: 0.9749\n",
            "trial number  2\n",
            "Epoch 1/3 | Train Loss: 0.5400 | Val Loss: 0.5673 | IoU: 0.1394 | Acc: 0.9646\n",
            "Epoch 2/3 | Train Loss: 0.4011 | Val Loss: 0.4078 | IoU: 0.1587 | Acc: 0.9668\n",
            "Epoch 3/3 | Train Loss: 0.3464 | Val Loss: 0.3636 | IoU: 0.1892 | Acc: 0.9674\n",
            "trial number  3\n",
            "Epoch 1/3 | Train Loss: 0.2342 | Val Loss: 0.2268 | IoU: 0.1520 | Acc: 0.9769\n",
            "Epoch 2/3 | Train Loss: 0.1469 | Val Loss: 0.2111 | IoU: 0.1534 | Acc: 0.9730\n",
            "Epoch 3/3 | Train Loss: 0.1359 | Val Loss: 0.2064 | IoU: 0.1574 | Acc: 0.9721\n",
            "{'lr': 6.868493510473524e-05, 'batch_size': 16, 'optimizer': 'adam', 'weight_decay': 0.0001, 'num_epochs': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "best_params_vt = random_search(model_type = \"vit\", device = device)\n",
        "print(best_params_vt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjOPke8ZUVdj",
        "outputId": "33c04755-7671-47bb-bcf4-cda77897d1c2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "trial number  1\n",
            "Epoch 1/3 | Train Loss: 0.2675 | Val Loss: 0.2569 | IoU: 0.0623 | Acc: 0.9815\n",
            "Epoch 2/3 | Train Loss: 0.2007 | Val Loss: 0.2470 | IoU: 0.0835 | Acc: 0.9723\n",
            "Epoch 3/3 | Train Loss: 0.1836 | Val Loss: 0.2327 | IoU: 0.1054 | Acc: 0.9739\n",
            "trial number  2\n",
            "Epoch 1/3 | Train Loss: 0.6622 | Val Loss: 0.5883 | IoU: 0.0097 | Acc: 0.8318\n",
            "Epoch 2/3 | Train Loss: 0.5007 | Val Loss: 0.4831 | IoU: 0.0063 | Acc: 0.9458\n",
            "Epoch 3/3 | Train Loss: 0.4135 | Val Loss: 0.4209 | IoU: 0.0326 | Acc: 0.9711\n",
            "trial number  3\n",
            "Epoch 1/3 | Train Loss: 0.7305 | Val Loss: 0.7183 | IoU: 0.0143 | Acc: 0.6093\n",
            "Epoch 2/3 | Train Loss: 0.6772 | Val Loss: 0.6771 | IoU: 0.0144 | Acc: 0.6786\n",
            "Epoch 3/3 | Train Loss: 0.6386 | Val Loss: 0.6430 | IoU: 0.0144 | Acc: 0.7365\n",
            "{'lr': 7.827837433030308e-05, 'batch_size': 64, 'patch_size': 32, 'n_heads': 4, 'optimizer': 'adam', 'weight_decay': 0.0005508653454252114, 'num_epochs': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next steps:\n",
        "# 1. Run hyperparametter tuning on 8 epochs for 8 trials\n",
        "# 2. Train both models for 20 epochs with early stopping\n",
        "# 3. Visualize attemtion maps\n",
        "# 4. Visualize filters\n",
        "# 5. Identify misclassified samples\n",
        "# 6. Test performance with merging the \"uncertainty\" class into fire/no-fire\n"
      ],
      "metadata": {
        "id": "e2ylGW19UchS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 1726926,
          "sourceId": 2824184,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
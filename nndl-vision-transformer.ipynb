{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2824184,"sourceType":"datasetVersion","datasetId":1726926}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport re\nfrom typing import Dict, List, Optional, Text, Tuple\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n\nimport tensorflow as tf\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:47.378134Z","iopub.execute_input":"2025-05-08T03:42:47.378845Z","iopub.status.idle":"2025-05-08T03:42:47.382851Z","shell.execute_reply.started":"2025-05-08T03:42:47.378820Z","shell.execute_reply":"2025-05-08T03:42:47.382183Z"}},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":"# load data","metadata":{}},{"cell_type":"code","source":"\"\"\"Constants for the data reader.\"\"\"\n\nINPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph', \n                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n\nOUTPUT_FEATURES = ['FireMask', ]\n\n# Data statistics \n# For each variable, the statistics are ordered in the form:\n# (min_clip, max_clip, mean, standard deviation)\nDATA_STATS = {\n    # Elevation in m.\n    # 0.1 percentile, 99.9 percentile\n    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n    # Pressure\n    # 0.1 percentile, 99.9 percentile\n    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),  # min, max\n    # Precipitation in mm.\n    # Negative values do not make sense, so min is set to 0.\n    # 0., 99.9 percentile\n    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n    # Specific humidity.\n    # Negative values do not make sense, so min is set to 0.\n    # The range of specific humidity is up to 100% so max is 1.\n    'sph': (0., 1., 0.0071658953, 0.0042835088),\n    # Wind direction in degrees clockwise from north.\n    # Thus min set to 0 and max set to 360.\n    'th': (0., 360.0, 190.32976, 72.59854),\n    # Min/max temperature in Kelvin.\n    # -20 degree C, 99.9 percentile\n    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n    # -20 degree C, 99.9 percentile\n    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n    # Wind speed in m/s.\n    # Negative values do not make sense, given there is a wind direction.\n    # 0., 99.9 percentile\n    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n    # NFDRS fire danger index energy release component expressed in BTU's per\n    # square foot.\n    # Negative values do not make sense. Thus min set to zero.\n    # 0., 99.9 percentile\n    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n    # Population density\n    # min, 99.9 percentile\n    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n    # We don't want to normalize the FireMasks.\n    # 1 indicates fire, 0 no fire, -1 unlabeled data\n    'PrevFireMask': (-1., 1., 0., 1.),\n    'FireMask': (-1., 1., 0., 1.)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:47.384211Z","iopub.execute_input":"2025-05-08T03:42:47.384598Z","iopub.status.idle":"2025-05-08T03:42:47.399055Z","shell.execute_reply.started":"2025-05-08T03:42:47.384580Z","shell.execute_reply":"2025-05-08T03:42:47.398486Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"\"\"\"Library of common functions used in deep learning neural networks.\n\"\"\"\ndef random_crop_input_and_output_images(\n    input_img: tf.Tensor,\n    output_img: tf.Tensor,\n    sample_size: int,\n    num_in_channels: int,\n    num_out_channels: int,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n  \"\"\"Randomly axis-align crop input and output image tensors.\n\n  Args:\n    input_img: tensor with dimensions HWC.\n    output_img: tensor with dimensions HWC.\n    sample_size: side length (square) to crop to.\n    num_in_channels: number of channels in input_img.\n    num_out_channels: number of channels in output_img.\n  Returns:\n    input_img: tensor with dimensions HWC.\n    output_img: tensor with dimensions HWC.\n  \"\"\"\n  combined = tf.concat([input_img, output_img], axis=2)\n  combined = tf.image.random_crop(\n      combined,\n      [sample_size, sample_size, num_in_channels + num_out_channels])\n  input_img = combined[:, :, 0:num_in_channels]\n  output_img = combined[:, :, -num_out_channels:]\n  return input_img, output_img\n\n\ndef center_crop_input_and_output_images(\n    input_img: tf.Tensor,\n    output_img: tf.Tensor,\n    sample_size: int,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n  \"\"\"Center crops input and output image tensors.\n\n  Args:\n    input_img: tensor with dimensions HWC.\n    output_img: tensor with dimensions HWC.\n    sample_size: side length (square) to crop to.\n  Returns:\n    input_img: tensor with dimensions HWC.\n    output_img: tensor with dimensions HWC.\n  \"\"\"\n  central_fraction = sample_size / input_img.shape[0]\n  input_img = tf.image.central_crop(input_img, central_fraction)\n  output_img = tf.image.central_crop(output_img, central_fraction)\n  return input_img, output_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:47.399670Z","iopub.execute_input":"2025-05-08T03:42:47.399868Z","iopub.status.idle":"2025-05-08T03:42:47.412637Z","shell.execute_reply.started":"2025-05-08T03:42:47.399854Z","shell.execute_reply":"2025-05-08T03:42:47.411933Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"\"\"\"Dataset reader for Earth Engine data.\"\"\"\n\ndef _get_base_key(key: Text) -> Text:\n  \"\"\"Extracts the base key from the provided key.\n\n  Earth Engine exports TFRecords containing each data variable with its\n  corresponding variable name. In the case of time sequences, the name of the\n  data variable is of the form 'variable_1', 'variable_2', ..., 'variable_n',\n  where 'variable' is the name of the variable, and n the number of elements\n  in the time sequence. Extracting the base key ensures that each step of the\n  time sequence goes through the same normalization steps.\n  The base key obeys the following naming pattern: '([a-zA-Z]+)'\n  For instance, for an input key 'variable_1', this function returns 'variable'.\n  For an input key 'variable', this function simply returns 'variable'.\n\n  Args:\n    key: Input key.\n\n  Returns:\n    The corresponding base key.\n\n  Raises:\n    ValueError when `key` does not match the expected pattern.\n  \"\"\"\n  match = re.match(r'([a-zA-Z]+)', key)\n  if match:\n    return match.group(1)\n  raise ValueError(\n      'The provided key does not match the expected pattern: {}'.format(key))\n\n\ndef _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n  \"\"\"Clips and rescales inputs with the stats corresponding to `key`.\n\n  Args:\n    inputs: Inputs to clip and rescale.\n    key: Key describing the inputs.\n\n  Returns:\n    Clipped and rescaled input.\n\n  Raises:\n    ValueError if there are no data statistics available for `key`.\n  \"\"\"\n  base_key = _get_base_key(key)\n  if base_key not in DATA_STATS:\n    raise ValueError(\n        'No data statistics available for the requested key: {}.'.format(key))\n  min_val, max_val, _, _ = DATA_STATS[base_key]\n  inputs = tf.clip_by_value(inputs, min_val, max_val)\n  return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n\n\ndef _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n  \"\"\"Clips and normalizes inputs with the stats corresponding to `key`.\n\n  Args:\n    inputs: Inputs to clip and normalize.\n    key: Key describing the inputs.\n\n  Returns:\n    Clipped and normalized input.\n\n  Raises:\n    ValueError if there are no data statistics available for `key`.\n  \"\"\"\n  base_key = _get_base_key(key)\n  if base_key not in DATA_STATS:\n    raise ValueError(\n        'No data statistics available for the requested key: {}.'.format(key))\n  min_val, max_val, mean, std = DATA_STATS[base_key]\n  inputs = tf.clip_by_value(inputs, min_val, max_val)\n  inputs = inputs - mean\n  return tf.math.divide_no_nan(inputs, std)\n\ndef _get_features_dict(\n    sample_size: int,\n    features: List[Text],\n) -> Dict[Text, tf.io.FixedLenFeature]:\n  \"\"\"Creates a features dictionary for TensorFlow IO.\n\n  Args:\n    sample_size: Size of the input tiles (square).\n    features: List of feature names.\n\n  Returns:\n    A features dictionary for TensorFlow IO.\n  \"\"\"\n  sample_shape = [sample_size, sample_size]\n  features = set(features)\n  columns = [\n      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n      for _ in features\n  ]\n  return dict(zip(features, columns))\n\n\ndef _parse_fn(\n    example_proto: tf.train.Example, data_size: int, sample_size: int,\n    num_in_channels: int, clip_and_normalize: bool,\n    clip_and_rescale: bool, random_crop: bool, center_crop: bool,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n  \"\"\"Reads a serialized example.\n\n  Args:\n    example_proto: A TensorFlow example protobuf.\n    data_size: Size of tiles (square) as read from input files.\n    sample_size: Size the tiles (square) when input into the model.\n    num_in_channels: Number of input channels.\n    clip_and_normalize: True if the data should be clipped and normalized.\n    clip_and_rescale: True if the data should be clipped and rescaled.\n    random_crop: True if the data should be randomly cropped.\n    center_crop: True if the data should be cropped in the center.\n\n  Returns:\n    (input_img, output_img) tuple of inputs and outputs to the ML model.\n  \"\"\"\n  if (random_crop and center_crop):\n    raise ValueError('Cannot have both random_crop and center_crop be True')\n  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n  feature_names = input_features + output_features\n  features_dict = _get_features_dict(data_size, feature_names)\n  features = tf.io.parse_single_example(example_proto, features_dict)\n\n  if clip_and_normalize:\n    inputs_list = [\n        _clip_and_normalize(features.get(key), key) for key in input_features\n    ]\n  elif clip_and_rescale:\n    inputs_list = [\n        _clip_and_rescale(features.get(key), key) for key in input_features\n    ]\n  else:\n    inputs_list = [features.get(key) for key in input_features]\n  \n  inputs_stacked = tf.stack(inputs_list, axis=0)\n  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n\n  outputs_list = [features.get(key) for key in output_features]\n  assert outputs_list, 'outputs_list should not be empty'\n  outputs_stacked = tf.stack(outputs_list, axis=0)\n\n  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n                                            'but dimensions of outputs_stacked'\n                                            f' are {outputs_stacked_shape}')\n  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n\n  if random_crop:\n    input_img, output_img = random_crop_input_and_output_images(\n        input_img, output_img, sample_size, num_in_channels, 1)\n  if center_crop:\n    input_img, output_img = center_crop_input_and_output_images(\n        input_img, output_img, sample_size)\n  return input_img, output_img\n\n\ndef get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n                batch_size: int, num_in_channels: int, compression_type: Text,\n                clip_and_normalize: bool, clip_and_rescale: bool,\n                random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n  \"\"\"Gets the dataset from the file pattern.\n\n  Args:\n    file_pattern: Input file pattern.\n    data_size: Size of tiles (square) as read from input files.\n    sample_size: Size the tiles (square) when input into the model.\n    batch_size: Batch size.\n    num_in_channels: Number of input channels.\n    compression_type: Type of compression used for the input files.\n    clip_and_normalize: True if the data should be clipped and normalized, False\n      otherwise.\n    clip_and_rescale: True if the data should be clipped and rescaled, False\n      otherwise.\n    random_crop: True if the data should be randomly cropped.\n    center_crop: True if the data shoulde be cropped in the center.\n\n  Returns:\n    A TensorFlow dataset loaded from the input file pattern, with features\n    described in the constants, and with the shapes determined from the input\n    parameters to this function.\n  \"\"\"\n  if (clip_and_normalize and clip_and_rescale):\n    raise ValueError('Cannot have both normalize and rescale.')\n  dataset = tf.data.Dataset.list_files(file_pattern)\n  dataset = dataset.interleave(\n      lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n  dataset = dataset.map(\n      lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n          x, data_size, sample_size, num_in_channels, clip_and_normalize,\n          clip_and_rescale, random_crop, center_crop),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  dataset = dataset.batch(batch_size)\n  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n  return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:47.414004Z","iopub.execute_input":"2025-05-08T03:42:47.414210Z","iopub.status.idle":"2025-05-08T03:42:47.434671Z","shell.execute_reply.started":"2025-05-08T03:42:47.414189Z","shell.execute_reply":"2025-05-08T03:42:47.434024Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"BATCH_SIZE = 32\nSAMPLE_SIZE = 64\n\ntrain_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_train*', \n    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n    clip_and_rescale=False, random_crop=True, center_crop=False)\n\nvalidation_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_eval*', \n    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n    clip_and_rescale=False, random_crop=True, center_crop=False)\n\ntest_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_test*',\n    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n    clip_and_rescale=False, random_crop=True, center_crop=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:47.435274Z","iopub.execute_input":"2025-05-08T03:42:47.435483Z","iopub.status.idle":"2025-05-08T03:42:48.608478Z","shell.execute_reply.started":"2025-05-08T03:42:47.435441Z","shell.execute_reply":"2025-05-08T03:42:48.607853Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"for x, y in train_dataset.take(1):\n    print(x.shape, y.shape)\n\nfor x, y in validation_dataset.take(1):\n    print(x.shape, y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:48.609287Z","iopub.execute_input":"2025-05-08T03:42:48.609525Z","iopub.status.idle":"2025-05-08T03:42:48.803212Z","shell.execute_reply.started":"2025-05-08T03:42:48.609507Z","shell.execute_reply":"2025-05-08T03:42:48.802437Z"}},"outputs":[{"name":"stdout","text":"(32, 64, 64, 12) (32, 64, 64, 1)\n(32, 64, 64, 12) (32, 64, 64, 1)\n","output_type":"stream"}],"execution_count":72},{"cell_type":"markdown","source":"# convert to torch ( i dont know how to tensor flow)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport torch\nfrom torch.utils.data import Dataset\n\n\n\nclass TFToTorchDataset(Dataset):\n    def __init__(self, tf_dataset, clean=False):\n        self.samples = []\n        for x, y in tf_dataset.as_numpy_iterator():\n            for i in range(x.shape[0]):\n                # Convert x: (32, 32, 12) → (12, 32, 32)\n                x_i = tf.transpose(x[i], perm=[2, 0, 1]).numpy()\n                # Convert y: (32, 32, 1) → (1, 32, 32)\n                y_i = tf.transpose(y[i], perm=[2, 0, 1]).numpy()\n                \n                if clean:\n                    if (y_i == -1).any():\n                        continue  # Skip this sample\n                self.samples.append((\n                    torch.tensor(x_i, dtype=torch.float32),\n                    torch.tensor(y_i, dtype=torch.float32)\n                ))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:48.804485Z","iopub.execute_input":"2025-05-08T03:42:48.804682Z","iopub.status.idle":"2025-05-08T03:42:48.810853Z","shell.execute_reply.started":"2025-05-08T03:42:48.804667Z","shell.execute_reply":"2025-05-08T03:42:48.810043Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"torch_dataset = TFToTorchDataset(train_dataset, clean=True)\ntrain_loader = torch.utils.data.DataLoader(torch_dataset, batch_size=32, shuffle=True)\n\ntorch_dataset_val =  TFToTorchDataset(validation_dataset, clean=True)\nval_loader = torch.utils.data.DataLoader(torch_dataset_val, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:48.813076Z","iopub.execute_input":"2025-05-08T03:42:48.813267Z","iopub.status.idle":"2025-05-08T03:43:21.087737Z","shell.execute_reply.started":"2025-05-08T03:42:48.813253Z","shell.execute_reply":"2025-05-08T03:43:21.087110Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"N = 5   \ndataiter = iter(train_loader)\n\nimage_list = []\nlabel_list = []\n#assume batch size equal to 1, otherwise divide N by batch size\nfor i in range(0, N): \n  image, label = next(dataiter)\n  image_list.append(image)\n  label_list.append(label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:43:21.091267Z","iopub.execute_input":"2025-05-08T03:43:21.091532Z","iopub.status.idle":"2025-05-08T03:43:21.166586Z","shell.execute_reply.started":"2025-05-08T03:43:21.091514Z","shell.execute_reply":"2025-05-08T03:43:21.165801Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"print(len(label_list[0][0][0][0]))\n# print(label_list[0][0])\nplt.imshow(label_list[4][0][0], cmap='viridis', interpolation='nearest')\nplt.colorbar()  \nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:43:21.167460Z","iopub.execute_input":"2025-05-08T03:43:21.167691Z","iopub.status.idle":"2025-05-08T03:43:21.350637Z","shell.execute_reply.started":"2025-05-08T03:43:21.167674Z","shell.execute_reply":"2025-05-08T03:43:21.349869Z"}},"outputs":[{"name":"stdout","text":"64\ntensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":76},{"cell_type":"markdown","source":"# fully conv model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=12, out_channels=1):\n        super(UNet, self).__init__()\n\n        def conv_block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_c),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_c),\n                nn.ReLU(inplace=True),\n            )\n\n        self.encoder1 = conv_block(in_channels, 64)\n        self.encoder2 = conv_block(64, 128)\n        self.encoder3 = conv_block(128, 256)\n\n        self.pool = nn.MaxPool2d(2)\n\n        self.bottleneck = conv_block(256, 512)\n\n        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.decoder3 = conv_block(512, 256)\n\n        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.decoder2 = conv_block(256, 128)\n\n        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.decoder1 = conv_block(128, 64)\n\n        self.output_layer = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        # Encoder\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(self.pool(e1))\n        e3 = self.encoder3(self.pool(e2))\n\n        # Bottleneck\n        b = self.bottleneck(self.pool(e3))\n\n        # Decoder\n        d3 = self.up3(b)\n        d3 = self.decoder3(torch.cat([d3, e3], dim=1))\n\n        d2 = self.up2(d3)\n        d2 = self.decoder2(torch.cat([d2, e2], dim=1))\n\n        d1 = self.up1(d2)\n        d1 = self.decoder1(torch.cat([d1, e1], dim=1))\n\n        # return torch.sigmoid(self.output_layer(d1))\n        return self.output_layer(d1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:43:21.351533Z","iopub.execute_input":"2025-05-08T03:43:21.351988Z","iopub.status.idle":"2025-05-08T03:43:21.360260Z","shell.execute_reply.started":"2025-05-08T03:43:21.351962Z","shell.execute_reply":"2025-05-08T03:43:21.359497Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef show_prediction(model, val_loader):\n    model.eval()\n    with torch.no_grad():\n        x_val, y_val = next(iter(val_loader))\n        x_val, y_val = x_val.to(device), y_val.to(device)\n        y_val = (y_val == 1).float()\n\n        pred = model(x_val)\n        pred_bin = (pred > 0.5).float()\n\n        # Show first sample\n        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n        axs[0].imshow(x_val[0, 11].cpu(), cmap='gray')\n        axs[0].set_title(\"Previous Fire Mask\")\n        axs[1].imshow(y_val[0, 0].cpu(), cmap='gray')\n        axs[1].set_title(\"Ground Truth\")\n        axs[2].imshow(pred_bin[0, 0].cpu(), cmap='gray')\n        axs[2].set_title(\"Prediction\")\n        for ax in axs:\n            ax.axis('off')\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:43:21.360924Z","iopub.execute_input":"2025-05-08T03:43:21.361101Z","iopub.status.idle":"2025-05-08T03:43:21.378996Z","shell.execute_reply.started":"2025-05-08T03:43:21.361079Z","shell.execute_reply":"2025-05-08T03:43:21.378268Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"def compute_iou(pred, target, threshold=0.5, eps=1e-6):\n    pred_bin = (pred > threshold).float()\n    target_bin = (target > 0.5).float()\n\n    intersection = (pred_bin * target_bin).sum(dim=(1, 2, 3))\n    union = (pred_bin + target_bin - pred_bin * target_bin).sum(dim=(1, 2, 3))\n    iou = (intersection + eps) / (union + eps)\n    return iou.mean().item()\n\ndef compute_accuracy(pred, target, threshold=0.5):\n    pred_bin = (pred > threshold).float()\n    correct = (pred_bin == target).float()\n    return correct.mean().item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:43:21.379776Z","iopub.execute_input":"2025-05-08T03:43:21.380040Z","iopub.status.idle":"2025-05-08T03:43:21.393922Z","shell.execute_reply.started":"2025-05-08T03:43:21.380014Z","shell.execute_reply":"2025-05-08T03:43:21.393172Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = UNet().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# criterion = nn.BCELoss()\n# use instead bc we have class imbalance\nweight_value = 0.85/0.15 # TODO pull the real percent background, percent fire from the dataset, i just guess 15% fire\npos_weight = torch.tensor([weight_value], device=device) \ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n\n        pred = model(x)\n        loss = criterion(pred, y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * x.size(0)\n\n    train_loss /= len(train_loader.dataset)\n\n    # === Validation ===\n    model.eval()\n    val_loss = 0.0\n    total_iou = 0.0\n    total_acc = 0.0\n    n_samples = 0\n    \n    with torch.no_grad():\n        for x_val, y_val in val_loader:\n            x_val, y_val = x_val.to(device), y_val.to(device)\n            y_val = (y_val == 1).float()\n    \n            pred_val = torch.sigmoid(model(x_val))\n            loss = criterion(pred_val, y_val)\n            val_loss += loss.item() * x_val.size(0)\n    \n            # Metrics\n            batch_iou = compute_iou(pred_val, y_val)\n            batch_acc = compute_accuracy(pred_val, y_val)\n    \n            total_iou += batch_iou * x_val.size(0)\n            total_acc += batch_acc * x_val.size(0)\n            n_samples += x_val.size(0)\n    \n    val_loss /= len(val_loader.dataset)\n    mean_iou = total_iou / n_samples\n    mean_acc = total_acc / n_samples\n\n\n    print(f\"Epoch {epoch+1:2d} | Train Loss: {train_loss:.4f} | \"\n      f\"Val Loss: {val_loss:.4f} | IoU: {mean_iou:.4f} | Acc: {mean_acc:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:43:21.394662Z","iopub.execute_input":"2025-05-08T03:43:21.394843Z","iopub.status.idle":"2025-05-08T03:48:19.091634Z","shell.execute_reply.started":"2025-05-08T03:43:21.394830Z","shell.execute_reply":"2025-05-08T03:48:19.090969Z"}},"outputs":[{"name":"stdout","text":"Epoch  1 | Train Loss: 0.3343 | Val Loss: 0.8166 | IoU: 0.1834 | Acc: 0.9679\nEpoch  2 | Train Loss: 0.2051 | Val Loss: 0.7850 | IoU: 0.2273 | Acc: 0.9731\nEpoch  3 | Train Loss: 0.1584 | Val Loss: 0.7667 | IoU: 0.2270 | Acc: 0.9747\nEpoch  4 | Train Loss: 0.1330 | Val Loss: 0.7610 | IoU: 0.2240 | Acc: 0.9671\nEpoch  5 | Train Loss: 0.1179 | Val Loss: 0.7528 | IoU: 0.2363 | Acc: 0.9774\nEpoch  6 | Train Loss: 0.1081 | Val Loss: 0.7497 | IoU: 0.2424 | Acc: 0.9797\nEpoch  7 | Train Loss: 0.1003 | Val Loss: 0.7480 | IoU: 0.2432 | Acc: 0.9739\nEpoch  8 | Train Loss: 0.0934 | Val Loss: 0.7454 | IoU: 0.2530 | Acc: 0.9795\nEpoch  9 | Train Loss: 0.0887 | Val Loss: 0.7452 | IoU: 0.2511 | Acc: 0.9762\nEpoch 10 | Train Loss: 0.0832 | Val Loss: 0.7437 | IoU: 0.2487 | Acc: 0.9801\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"show_prediction(model, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:48:39.739283Z","iopub.execute_input":"2025-05-08T03:48:39.739987Z","iopub.status.idle":"2025-05-08T03:48:39.924659Z","shell.execute_reply.started":"2025-05-08T03:48:39.739966Z","shell.execute_reply":"2025-05-08T03:48:39.923909Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x400 with 3 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":87},{"cell_type":"markdown","source":"# vision transformer (IGNORE)","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torchvision.models.vision_transformer import VisionTransformer\n\n# class FireMaskViT(nn.Module):\n#     def __init__(self, input_channels, image_size=64, patch_size=8, num_classes=1, hidden_dim=768, num_layers=12, num_heads=12):\n#         super().__init__()\n\n#         assert image_size % patch_size == 0, \"image_size must be divisible by patch_size\"\n\n#         num_patches = (image_size // patch_size) ** 2\n        \n#         # VisionTransformer expects 3 input channels → we re-define input projection\n#         self.vit = VisionTransformer(\n#             image_size=image_size,\n#             patch_size=patch_size,\n#             num_layers=num_layers,\n#             num_heads=num_heads,\n#             hidden_dim=hidden_dim,\n#             mlp_dim=hidden_dim * 4,\n#             num_classes=0,  # no classification head\n#         )\n\n#         # Replace input projection layer to accept input_channels instead of 3\n#         self.vit.conv_proj = nn.Conv2d(input_channels, hidden_dim, kernel_size=patch_size, stride=patch_size)\n\n#         # self.vit.encoder.pos_embedding = nn.Parameter(torch.randn(1, num_patches, hidden_dim))\n#         self.vit.encoder.cls_token = None\n#         self.vit.encoder.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, hidden_dim))\n\n\n\n#         # Output: project transformer embedding back to patch predictions\n#         # num_patches = (image_size // patch_size) ** 2\n#         self.output_head = nn.Linear(hidden_dim, patch_size * patch_size)\n\n#     def forward(self, x):\n#         # x: (batch_size, C, 64, 64)\n#         embeddings = self.vit(x)  # (batch_size, num_patches, hidden_dim)\n#         out = self.output_head(embeddings)  # (batch_size, num_patches, patch_pixels)\n#         # Reshape output back to (batch_size, 1, 64, 64)\n#         B, N, P = out.shape\n#         patch_size = int(P ** 0.5)\n#         H = W = int(N ** 0.5)\n#         out = out.view(B, H, W, patch_size, patch_size).permute(0,1,3,2,4).reshape(B, 1, H*patch_size, W*patch_size)\n#         return out\n\n# # Example usage\n# input_channels = 12  # prev fire mask + NDVI + temp + humidity + ...\n# model = FireMaskViT(input_channels=input_channels)\n\n# dummy_input = torch.randn(2, input_channels, 64, 64)\n# output = model(dummy_input)  # → output shape: (2, 1, 64, 64)\n# print(output.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:16:47.645181Z","iopub.execute_input":"2025-05-08T04:16:47.645740Z","iopub.status.idle":"2025-05-08T04:16:48.909794Z","shell.execute_reply.started":"2025-05-08T04:16:47.645718Z","shell.execute_reply":"2025-05-08T04:16:48.908763Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/32640888.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# → output shape: (2, 1, 64, 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/32640888.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# x: (batch_size, C, 64, 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, num_patches, hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, num_patches, patch_pixels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Reshape output back to (batch_size, 1, 64, 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x0 and 768x64)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (2x0 and 768x64)","output_type":"error"}],"execution_count":97},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torchvision.models.vision_transformer import VisionTransformer\n\n# class FireMaskViT(nn.Module):\n#     def __init__(self, input_channels=12, image_size=64, patch_size=8, hidden_dim=768, num_classes=1):\n#         super().__init__()\n\n#         assert image_size % patch_size == 0, \"image_size must be divisible by patch_size\"\n#         num_patches = (image_size // patch_size) ** 2\n\n#         # 1. Load pretrained ViT or define from scratch\n#         self.vit = VisionTransformer(\n#             image_size=image_size,\n#             patch_size=patch_size,\n#             num_layers=12,\n#             num_heads=12,\n#             hidden_dim=hidden_dim,\n#             mlp_dim=hidden_dim * 4,\n#             num_classes=0,  # No classifier head\n#         )\n\n#         # 2. Replace input stem to accept 12 channels\n#         self.vit.conv_proj = nn.Conv2d(input_channels, hidden_dim, kernel_size=patch_size, stride=patch_size)\n\n#         # 3. Replace positional embedding with correct size\n#         self.vit.encoder.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, hidden_dim))\n\n#         # 4. Decoder head to produce patch_size x patch_size pixels per patch\n#         self.output_head = nn.Linear(hidden_dim, patch_size * patch_size)\n\n#         self.patch_size = patch_size\n#         self.image_size = image_size\n\n#     def forward(self, x):\n#         B = x.size(0)\n\n#         # x: (B, 12, 64, 64)\n#         x = self.vit(x)  # (B, num_patches, hidden_dim)\n#         x = self.output_head(x)  # (B, num_patches, patch_area)\n\n#         P = self.patch_size\n#         N = self.image_size // P\n#         out = x.view(B, N, N, P, P).permute(0, 1, 3, 2, 4).reshape(B, 1, self.image_size, self.image_size)\n#         return out\n\n\n# model = FireMaskViT(input_channels=12, image_size=64, patch_size=8)\n# dummy_input = torch.randn(2, 12, 64, 64)\n# out = model(dummy_input)\n# print(out.shape)  # Expected: torch.Size([2, 1, 64, 64])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:15:25.086572Z","iopub.execute_input":"2025-05-08T04:15:25.087266Z","iopub.status.idle":"2025-05-08T04:15:26.227469Z","shell.execute_reply.started":"2025-05-08T04:15:25.087241Z","shell.execute_reply":"2025-05-08T04:15:26.226456Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/796239275.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFireMaskViT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Expected: torch.Size([2, 1, 64, 64])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/796239275.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# x: (B, 12, 64, 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, num_patches, hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, num_patches, patch_area)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x0 and 768x64)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (2x0 and 768x64)","output_type":"error"}],"execution_count":95},{"cell_type":"markdown","source":"# timm vt","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\n\nclass FireMaskViT(nn.Module):\n    def __init__(self, input_channels=12, image_size=64, patch_size=8, hidden_dim=768):\n        super().__init__()\n\n        assert image_size % patch_size == 0, \"Image size must be divisible by patch size.\"\n\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.hidden_dim = hidden_dim\n        self.num_patches = (image_size // patch_size) ** 2\n\n        self.vit = timm.create_model(\n            'vit_base_patch16_224',\n            pretrained=False,\n            img_size=image_size,\n            patch_size=patch_size,\n            in_chans=input_channels,\n            num_classes=0  # no classifier head\n        )\n\n        # print(type(self.vit))\n\n\n        self.output_head = nn.Linear(self.vit.embed_dim, patch_size * patch_size)\n\n    def forward(self, x):\n        B = x.shape[0]\n\n        x = self.vit.forward_features(x)\n        x = x[:, 1:, :]    \n        # print(\"vit output:\", x.shape)\n\n        x = self.output_head(x)  # shape (B, num_patches, patch_area)\n\n        P = self.patch_size\n        N = self.image_size // P  # patches along each dimension\n\n        x = x.view(B, N, N, P, P).permute(0, 1, 3, 2, 4).reshape(B, 1, self.image_size, self.image_size)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:39:51.562916Z","iopub.execute_input":"2025-05-08T04:39:51.563178Z","iopub.status.idle":"2025-05-08T04:39:51.570047Z","shell.execute_reply.started":"2025-05-08T04:39:51.563159Z","shell.execute_reply":"2025-05-08T04:39:51.569321Z"}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"# model = FireMaskViT(input_channels=12, image_size=64, patch_size=8)\n# dummy_input = torch.randn(32, 12, 64, 64)\n# output = model(dummy_input)\n# print(output.shape)  # Should be: torch.Size([32, 1, 64, 64])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:38:15.841758Z","iopub.execute_input":"2025-05-08T04:38:15.842036Z","iopub.status.idle":"2025-05-08T04:38:19.231225Z","shell.execute_reply.started":"2025-05-08T04:38:15.842015Z","shell.execute_reply":"2025-05-08T04:38:19.230502Z"}},"outputs":[{"name":"stdout","text":"<class 'timm.models.vision_transformer.VisionTransformer'>\nvit output: torch.Size([32, 64, 768])\ntorch.Size([32, 1, 64, 64])\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = FireMaskViT(input_channels=12).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\npos_weight = torch.tensor([10.0]).to(device)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n# criterion = nn.BCEWithLogitsLoss()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:51:36.379105Z","iopub.execute_input":"2025-05-08T04:51:36.379663Z","iopub.status.idle":"2025-05-08T04:51:37.626162Z","shell.execute_reply.started":"2025-05-08T04:51:36.379642Z","shell.execute_reply":"2025-05-08T04:51:37.625581Z"}},"outputs":[],"execution_count":131},{"cell_type":"code","source":"for epoch in range(1, 5):\n    model.train()\n    train_loss = 0\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    \n    val_loss = 0\n    model.eval()\n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            val_loss += loss.item()\n\n    print(f\"Epoch {epoch:2d} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:51:39.502542Z","iopub.execute_input":"2025-05-08T04:51:39.503161Z","iopub.status.idle":"2025-05-08T04:59:41.420417Z","shell.execute_reply.started":"2025-05-08T04:51:39.503141Z","shell.execute_reply":"2025-05-08T04:59:41.419578Z"}},"outputs":[{"name":"stdout","text":"Epoch  1 | Train Loss: 0.2656 | Val Loss: 0.2947\nEpoch  2 | Train Loss: 0.1905 | Val Loss: 0.2780\nEpoch  3 | Train Loss: 0.1719 | Val Loss: 0.2687\nEpoch  4 | Train Loss: 0.1636 | Val Loss: 0.2587\n","output_type":"stream"}],"execution_count":132},{"cell_type":"code","source":"def evaluate(model, dataloader, threshold=0.5):\n    model.eval()\n    correct, total = 0, 0\n    intersection, union = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = torch.sigmoid(model(X)) > threshold\n            correct += (pred == y).sum().item()\n            total += torch.numel(y)\n            intersection += (pred & (y > 0.5)).sum().item()\n            union += ((pred | (y > 0.5))).sum().item()\n    acc = correct / total\n    iou = intersection / union if union else 0\n    print(f\"Pixel Accuracy: {acc:.4f}, IoU: {iou:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T05:00:21.535688Z","iopub.execute_input":"2025-05-08T05:00:21.536360Z","iopub.status.idle":"2025-05-08T05:00:21.541640Z","shell.execute_reply.started":"2025-05-08T05:00:21.536340Z","shell.execute_reply":"2025-05-08T05:00:21.540837Z"}},"outputs":[],"execution_count":137},{"cell_type":"code","source":"show_prediction(model, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:59:52.117806Z","iopub.execute_input":"2025-05-08T04:59:52.118467Z","iopub.status.idle":"2025-05-08T04:59:53.030251Z","shell.execute_reply.started":"2025-05-08T04:59:52.118429Z","shell.execute_reply":"2025-05-08T04:59:53.029593Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x400 with 3 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":136},{"cell_type":"code","source":"evaluate(model, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T05:00:34.208675Z","iopub.execute_input":"2025-05-08T05:00:34.209196Z","iopub.status.idle":"2025-05-08T05:00:39.030196Z","shell.execute_reply.started":"2025-05-08T05:00:34.209172Z","shell.execute_reply":"2025-05-08T05:00:39.029428Z"}},"outputs":[{"name":"stdout","text":"Pixel Accuracy: 0.9643, IoU: 0.1898\n","output_type":"stream"}],"execution_count":138}]}
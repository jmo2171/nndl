{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2824184,"sourceType":"datasetVersion","datasetId":1726926}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchcam","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:16:10.949120Z","iopub.execute_input":"2025-05-11T16:16:10.949785Z","iopub.status.idle":"2025-05-11T16:16:14.043915Z","shell.execute_reply.started":"2025-05-11T16:16:10.949760Z","shell.execute_reply":"2025-05-11T16:16:14.042932Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchcam in /usr/local/lib/python3.11/dist-packages (0.4.0)\nRequirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchcam) (2.5.1+cu124)\nRequirement already satisfied: numpy<2.0.0,>=1.17.2 in /usr/local/lib/python3.11/dist-packages (from torchcam) (1.26.4)\nRequirement already satisfied: Pillow!=9.2.0,>=8.4.0 in /usr/local/lib/python3.11/dist-packages (from torchcam) (11.1.0)\nRequirement already satisfied: matplotlib<4.0.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from torchcam) (3.7.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.17.2->torchcam) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.17.2->torchcam) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.17.2->torchcam) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.17.2->torchcam) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.17.2->torchcam) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.17.2->torchcam) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.0.0->torchcam) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.0->torchcam) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0.0,>=2.0.0->torchcam) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.17.2->torchcam) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.17.2->torchcam) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.17.2->torchcam) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0.0,>=1.17.2->torchcam) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0.0,>=1.17.2->torchcam) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport re\nfrom typing import Dict, List, Optional, Text, Tuple\nimport torchcam\nfrom torchcam.methods import GradCAM\nfrom torchcam.utils import overlay_mask\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n\nimport tensorflow as tf\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-05-11T16:16:14.045481Z","iopub.execute_input":"2025-05-11T16:16:14.045749Z","iopub.status.idle":"2025-05-11T16:16:19.062982Z","shell.execute_reply.started":"2025-05-11T16:16:14.045726Z","shell.execute_reply":"2025-05-11T16:16:19.062351Z"},"trusted":true,"id":"nstcz6fNGq0_"},"outputs":[{"name":"stderr","text":"2025-05-11 16:16:16.238292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746980176.261170     222 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746980176.268404     222 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# load data","metadata":{"id":"lW72d6GUGq1A"}},{"cell_type":"code","source":"\"\"\"Constants for the data reader.\"\"\"\n\nINPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph',\n                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n\nOUTPUT_FEATURES = ['FireMask', ]\n\n# Data statistics\n# For each variable, the statistics are ordered in the form:\n# (min_clip, max_clip, mean, standard deviation)\nDATA_STATS = {\n    # Elevation in m.\n    # 0.1 percentile, 99.9 percentile\n    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n    # Pressure\n    # 0.1 percentile, 99.9 percentile\n    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),  # min, max\n    # Precipitation in mm.\n    # Negative values do not make sense, so min is set to 0.\n    # 0., 99.9 percentile\n    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n    # Specific humidity.\n    # Negative values do not make sense, so min is set to 0.\n    # The range of specific humidity is up to 100% so max is 1.\n    'sph': (0., 1., 0.0071658953, 0.0042835088),\n    # Wind direction in degrees clockwise from north.\n    # Thus min set to 0 and max set to 360.\n    'th': (0., 360.0, 190.32976, 72.59854),\n    # Min/max temperature in Kelvin.\n    # -20 degree C, 99.9 percentile\n    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n    # -20 degree C, 99.9 percentile\n    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n    # Wind speed in m/s.\n    # Negative values do not make sense, given there is a wind direction.\n    # 0., 99.9 percentile\n    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n    # NFDRS fire danger index energy release component expressed in BTU's per\n    # square foot.\n    # Negative values do not make sense. Thus min set to zero.\n    # 0., 99.9 percentile\n    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n    # Population density\n    # min, 99.9 percentile\n    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n    # We don't want to normalize the FireMasks.\n    # 1 indicates fire, 0 no fire, -1 unlabeled data\n    'PrevFireMask': (-1., 1., 0., 1.),\n    'FireMask': (-1., 1., 0., 1.)\n}","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:16:19.064326Z","iopub.execute_input":"2025-05-11T16:16:19.065180Z","iopub.status.idle":"2025-05-11T16:16:19.072088Z","shell.execute_reply.started":"2025-05-11T16:16:19.065159Z","shell.execute_reply":"2025-05-11T16:16:19.071134Z"},"trusted":true,"id":"7NQKNV0cGq1B"},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\"\"\"Library of common functions used in deep learning neural networks.\n\"\"\"\ndef random_crop_input_and_output_images(\n    input_img: tf.Tensor,\n    output_img: tf.Tensor,\n    sample_size: int,\n    num_in_channels: int,\n    num_out_channels: int,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n  \"\"\"Randomly axis-align crop input and output image tensors.\n\n  Args:\n    input_img: tensor with dimensions HWC.\n    output_img: tensor with dimensions HWC.\n    sample_size: side length (square) to crop to.\n    num_in_channels: number of channels in input_img.\n    num_out_channels: number of channels in output_img.\n  Returns:\n    input_img: tensor with dimensions HWC.\n    output_img: tensor with dimensions HWC.\n  \"\"\"\n  combined = tf.concat([input_img, output_img], axis=2)\n  combined = tf.image.random_crop(\n      combined,\n      [sample_size, sample_size, num_in_channels + num_out_channels])\n  input_img = combined[:, :, 0:num_in_channels]\n  output_img = combined[:, :, -num_out_channels:]\n  return input_img, output_img\n\n\ndef center_crop_input_and_output_images(\n    input_img: tf.Tensor,\n    output_img: tf.Tensor,\n    sample_size: int,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n  \"\"\"Center crops input and output image tensors.\n\n  Args:\n    input_img: tensor with dimensions HWC.\n    output_img: tensor with dimensions HWC.\n    sample_size: side length (square) to crop to.\n  Returns:\n    input_img: tensor with dimensions HWC.\n    output_img: tensor with dimensions HWC.\n  \"\"\"\n  central_fraction = sample_size / input_img.shape[0]\n  input_img = tf.image.central_crop(input_img, central_fraction)\n  output_img = tf.image.central_crop(output_img, central_fraction)\n  return input_img, output_img","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:16:19.072908Z","iopub.execute_input":"2025-05-11T16:16:19.073112Z","iopub.status.idle":"2025-05-11T16:16:19.091511Z","shell.execute_reply.started":"2025-05-11T16:16:19.073095Z","shell.execute_reply":"2025-05-11T16:16:19.090872Z"},"trusted":true,"id":"kO0KKjUVGq1C"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\"\"\"Dataset reader for Earth Engine data.\"\"\"\n\ndef _get_base_key(key: Text) -> Text:\n  \"\"\"Extracts the base key from the provided key.\n\n  Earth Engine exports TFRecords containing each data variable with its\n  corresponding variable name. In the case of time sequences, the name of the\n  data variable is of the form 'variable_1', 'variable_2', ..., 'variable_n',\n  where 'variable' is the name of the variable, and n the number of elements\n  in the time sequence. Extracting the base key ensures that each step of the\n  time sequence goes through the same normalization steps.\n  The base key obeys the following naming pattern: '([a-zA-Z]+)'\n  For instance, for an input key 'variable_1', this function returns 'variable'.\n  For an input key 'variable', this function simply returns 'variable'.\n\n  Args:\n    key: Input key.\n\n  Returns:\n    The corresponding base key.\n\n  Raises:\n    ValueError when `key` does not match the expected pattern.\n  \"\"\"\n  match = re.match(r'([a-zA-Z]+)', key)\n  if match:\n    return match.group(1)\n  raise ValueError(\n      'The provided key does not match the expected pattern: {}'.format(key))\n\n\ndef _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n  \"\"\"Clips and rescales inputs with the stats corresponding to `key`.\n\n  Args:\n    inputs: Inputs to clip and rescale.\n    key: Key describing the inputs.\n\n  Returns:\n    Clipped and rescaled input.\n\n  Raises:\n    ValueError if there are no data statistics available for `key`.\n  \"\"\"\n  base_key = _get_base_key(key)\n  if base_key not in DATA_STATS:\n    raise ValueError(\n        'No data statistics available for the requested key: {}.'.format(key))\n  min_val, max_val, _, _ = DATA_STATS[base_key]\n  inputs = tf.clip_by_value(inputs, min_val, max_val)\n  return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n\n\ndef _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n  \"\"\"Clips and normalizes inputs with the stats corresponding to `key`.\n\n  Args:\n    inputs: Inputs to clip and normalize.\n    key: Key describing the inputs.\n\n  Returns:\n    Clipped and normalized input.\n\n  Raises:\n    ValueError if there are no data statistics available for `key`.\n  \"\"\"\n  base_key = _get_base_key(key)\n  if base_key not in DATA_STATS:\n    raise ValueError(\n        'No data statistics available for the requested key: {}.'.format(key))\n  min_val, max_val, mean, std = DATA_STATS[base_key]\n  inputs = tf.clip_by_value(inputs, min_val, max_val)\n  inputs = inputs - mean\n  return tf.math.divide_no_nan(inputs, std)\n\ndef _get_features_dict(\n    sample_size: int,\n    features: List[Text],\n) -> Dict[Text, tf.io.FixedLenFeature]:\n  \"\"\"Creates a features dictionary for TensorFlow IO.\n\n  Args:\n    sample_size: Size of the input tiles (square).\n    features: List of feature names.\n\n  Returns:\n    A features dictionary for TensorFlow IO.\n  \"\"\"\n  sample_shape = [sample_size, sample_size]\n  features = set(features)\n  columns = [\n      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n      for _ in features\n  ]\n  return dict(zip(features, columns))\n\n\ndef _parse_fn(\n    example_proto: tf.train.Example, data_size: int, sample_size: int,\n    num_in_channels: int, clip_and_normalize: bool,\n    clip_and_rescale: bool, random_crop: bool, center_crop: bool,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n  \"\"\"Reads a serialized example.\n\n  Args:\n    example_proto: A TensorFlow example protobuf.\n    data_size: Size of tiles (square) as read from input files.\n    sample_size: Size the tiles (square) when input into the model.\n    num_in_channels: Number of input channels.\n    clip_and_normalize: True if the data should be clipped and normalized.\n    clip_and_rescale: True if the data should be clipped and rescaled.\n    random_crop: True if the data should be randomly cropped.\n    center_crop: True if the data should be cropped in the center.\n\n  Returns:\n    (input_img, output_img) tuple of inputs and outputs to the ML model.\n  \"\"\"\n  if (random_crop and center_crop):\n    raise ValueError('Cannot have both random_crop and center_crop be True')\n  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n  feature_names = input_features + output_features\n  features_dict = _get_features_dict(data_size, feature_names)\n  features = tf.io.parse_single_example(example_proto, features_dict)\n\n  if clip_and_normalize:\n    inputs_list = [\n        _clip_and_normalize(features.get(key), key) for key in input_features\n    ]\n  elif clip_and_rescale:\n    inputs_list = [\n        _clip_and_rescale(features.get(key), key) for key in input_features\n    ]\n  else:\n    inputs_list = [features.get(key) for key in input_features]\n\n  inputs_stacked = tf.stack(inputs_list, axis=0)\n  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n\n  outputs_list = [features.get(key) for key in output_features]\n  assert outputs_list, 'outputs_list should not be empty'\n  outputs_stacked = tf.stack(outputs_list, axis=0)\n\n  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n                                            'but dimensions of outputs_stacked'\n                                            f' are {outputs_stacked_shape}')\n  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n\n  if random_crop:\n    input_img, output_img = random_crop_input_and_output_images(\n        input_img, output_img, sample_size, num_in_channels, 1)\n  if center_crop:\n    input_img, output_img = center_crop_input_and_output_images(\n        input_img, output_img, sample_size)\n  return input_img, output_img\n\n\ndef get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n                batch_size: int, num_in_channels: int, compression_type: Text,\n                clip_and_normalize: bool, clip_and_rescale: bool,\n                random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n  \"\"\"Gets the dataset from the file pattern.\n\n  Args:\n    file_pattern: Input file pattern.\n    data_size: Size of tiles (square) as read from input files.\n    sample_size: Size the tiles (square) when input into the model.\n    batch_size: Batch size.\n    num_in_channels: Number of input channels.\n    compression_type: Type of compression used for the input files.\n    clip_and_normalize: True if the data should be clipped and normalized, False\n      otherwise.\n    clip_and_rescale: True if the data should be clipped and rescaled, False\n      otherwise.\n    random_crop: True if the data should be randomly cropped.\n    center_crop: True if the data shoulde be cropped in the center.\n\n  Returns:\n    A TensorFlow dataset loaded from the input file pattern, with features\n    described in the constants, and with the shapes determined from the input\n    parameters to this function.\n  \"\"\"\n  if (clip_and_normalize and clip_and_rescale):\n    raise ValueError('Cannot have both normalize and rescale.')\n  dataset = tf.data.Dataset.list_files(file_pattern)\n  dataset = dataset.interleave(\n      lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n  dataset = dataset.map(\n      lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n          x, data_size, sample_size, num_in_channels, clip_and_normalize,\n          clip_and_rescale, random_crop, center_crop),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  dataset = dataset.batch(batch_size)\n  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n  return dataset","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:16:19.093099Z","iopub.execute_input":"2025-05-11T16:16:19.093333Z","iopub.status.idle":"2025-05-11T16:16:19.110999Z","shell.execute_reply.started":"2025-05-11T16:16:19.093307Z","shell.execute_reply":"2025-05-11T16:16:19.110227Z"},"trusted":true,"id":"bIp_TLf5Gq1D"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"BATCH_SIZE = 32\nSAMPLE_SIZE = 64\n\ntrain_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_train*',\n    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n    clip_and_rescale=False, random_crop=True, center_crop=False)\n\nvalidation_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_eval*',\n    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n    clip_and_rescale=False, random_crop=True, center_crop=False)\n\ntest_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_test*',\n    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n    clip_and_rescale=False, random_crop=True, center_crop=False)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:16:19.111709Z","iopub.execute_input":"2025-05-11T16:16:19.111943Z","iopub.status.idle":"2025-05-11T16:16:20.569560Z","shell.execute_reply.started":"2025-05-11T16:16:19.111921Z","shell.execute_reply":"2025-05-11T16:16:20.569032Z"},"trusted":true,"id":"h5sYQrssGq1E"},"outputs":[{"name":"stderr","text":"I0000 00:00:1746980179.422781     222 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1746980179.423454     222 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### For loading the dataset from google drive","metadata":{"id":"cNCaryRXLJJY"}},{"cell_type":"code","source":"# import glob\n# from google.colab import drive\n\n# BATCH_SIZE = 32\n# SAMPLE_SIZE = 64\n\n# # Mount Google Drive\n# drive.mount('/content/drive')\n\n# # Use glob to find all matching files\n# train_files = glob.glob('/content/drive/MyDrive/wildfires/next_day_wildfire_spread_train*.tfrecord')\n# validation_files = glob.glob('/content/drive/MyDrive/wildfires/next_day_wildfire_spread_eval*.tfrecord')\n# test_files = glob.glob('/content/drive/MyDrive/wildfires/next_day_wildfire_spread_test*.tfrecord')\n\n# # Create the datasets using the list of files\n# train_dataset = get_dataset(train_files,\n#     data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n#     num_in_channels=12, compression_type=None, clip_and_normalize=True,\n#     clip_and_rescale=False, random_crop=True, center_crop=False)\n\n# validation_dataset = get_dataset(validation_files,\n#     data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n#     num_in_channels=12, compression_type=None, clip_and_normalize=True,\n#     clip_and_rescale=False, random_crop=True, center_crop=False)\n\n# test_dataset = get_dataset(test_files,\n#     data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n#     num_in_channels=12, compression_type=None, clip_and_normalize=True,\n#     clip_and_rescale=False, random_crop=True, center_crop=False)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhcJ1Tx4KgTC","outputId":"9e17e5b8-5c9b-493e-8a83-ecd74d592761","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:16:20.570243Z","iopub.execute_input":"2025-05-11T16:16:20.570452Z","iopub.status.idle":"2025-05-11T16:16:20.574106Z","shell.execute_reply.started":"2025-05-11T16:16:20.570435Z","shell.execute_reply":"2025-05-11T16:16:20.573356Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"for x, y in train_dataset.take(1):\n    print(x.shape, y.shape)\n\nfor x, y in validation_dataset.take(1):\n    print(x.shape, y.shape)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:16:20.574734Z","iopub.execute_input":"2025-05-11T16:16:20.574916Z","iopub.status.idle":"2025-05-11T16:16:20.789081Z","shell.execute_reply.started":"2025-05-11T16:16:20.574901Z","shell.execute_reply":"2025-05-11T16:16:20.788392Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"CbtymGfSGq1E","outputId":"f39aa7f6-8270-4be2-a92b-bdaedda9790d"},"outputs":[{"name":"stdout","text":"(32, 64, 64, 12) (32, 64, 64, 1)\n(32, 64, 64, 12) (32, 64, 64, 1)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# convert to torch ( i dont know how to tensor flow)","metadata":{"id":"cCGGbOfyGq1F"}},{"cell_type":"code","source":"import tensorflow as tf\nimport torch\nfrom torch.utils.data import Dataset\n\n\n\nclass TFToTorchDataset(Dataset):\n    def __init__(self, tf_dataset, clean=False):\n        self.samples = []\n        for x, y in tf_dataset.as_numpy_iterator():\n            for i in range(x.shape[0]):\n                # Convert x: (32, 32, 12) → (12, 32, 32)\n                x_i = tf.transpose(x[i], perm=[2, 0, 1]).numpy()\n                # Convert y: (32, 32, 1) → (1, 32, 32)\n                y_i = tf.transpose(y[i], perm=[2, 0, 1]).numpy()\n\n                if clean:\n                    if (y_i == -1).any():\n                        continue  # Skip this sample\n                self.samples.append((\n                    torch.tensor(x_i, dtype=torch.float32),\n                    torch.tensor(y_i, dtype=torch.float32)\n                ))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]\n","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:16:20.789839Z","iopub.execute_input":"2025-05-11T16:16:20.790050Z","iopub.status.idle":"2025-05-11T16:16:20.796087Z","shell.execute_reply.started":"2025-05-11T16:16:20.790033Z","shell.execute_reply":"2025-05-11T16:16:20.795374Z"},"trusted":true,"id":"3TZPz899Gq1F"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"torch_dataset = TFToTorchDataset(train_dataset, clean=True)\ntrain_loader = torch.utils.data.DataLoader(torch_dataset, batch_size=32, shuffle=True)\n\ntorch_dataset_val =  TFToTorchDataset(validation_dataset, clean=True)\nval_loader = torch.utils.data.DataLoader(torch_dataset_val, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:16:20.796666Z","iopub.execute_input":"2025-05-11T16:16:20.796901Z","iopub.status.idle":"2025-05-11T16:16:54.687809Z","shell.execute_reply.started":"2025-05-11T16:16:20.796884Z","shell.execute_reply":"2025-05-11T16:16:54.686993Z"},"trusted":true,"id":"dRnYCmj9Gq1F"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"N = 5\ndataiter = iter(train_loader)\n\nimage_list = []\nlabel_list = []\n#assume batch size equal to 1, otherwise divide N by batch size\nfor i in range(0, N):\n  image, label = next(dataiter)\n  image_list.append(image)\n  label_list.append(label)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:16:54.691126Z","iopub.execute_input":"2025-05-11T16:16:54.691418Z","iopub.status.idle":"2025-05-11T16:16:54.719383Z","shell.execute_reply.started":"2025-05-11T16:16:54.691396Z","shell.execute_reply":"2025-05-11T16:16:54.718659Z"},"trusted":true,"id":"e6HEdxDqGq1G"},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print(len(label_list[0][0][0][0]))\n# print(label_list[0][0])\nplt.imshow(label_list[4][0][0], cmap='viridis', interpolation='nearest')\nplt.colorbar()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:16:54.720322Z","iopub.execute_input":"2025-05-11T16:16:54.720633Z","iopub.status.idle":"2025-05-11T16:16:54.943628Z","shell.execute_reply.started":"2025-05-11T16:16:54.720606Z","shell.execute_reply":"2025-05-11T16:16:54.942864Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"JNvEtlxvGq1G","outputId":"60990b13-f9f9-4857-e133-8bc6de332181"},"outputs":[{"name":"stdout","text":"64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"plt.imshow(label_list[4][1][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:16:54.944370Z","iopub.execute_input":"2025-05-11T16:16:54.944657Z","iopub.status.idle":"2025-05-11T16:16:55.104511Z","shell.execute_reply.started":"2025-05-11T16:16:54.944631Z","shell.execute_reply":"2025-05-11T16:16:55.103671Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7a5f74101f90>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Helper functions","metadata":{"id":"O9Ud8xSQGq1H"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef show_prediction(model, val_loader):\n    model.eval()\n    with torch.no_grad():\n        x_val, y_val = next(iter(val_loader))\n        x_val, y_val = x_val.to(device), y_val.to(device)\n        y_val = (y_val == 1).float()\n\n        pred = model(x_val)\n        pred_bin = (pred > 0.5).float()\n\n        # Show first sample\n        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n        axs[0].imshow(x_val[0, 11].cpu(), cmap='gray')\n        axs[0].set_title(\"Previous Fire Mask\")\n        axs[1].imshow(y_val[0, 0].cpu(), cmap='gray')\n        axs[1].set_title(\"Ground Truth\")\n        axs[2].imshow(pred_bin[0, 0].cpu(), cmap='gray')\n        axs[2].set_title(\"Prediction\")\n        for ax in axs:\n            ax.axis('off')\n        plt.show()\n","metadata":{"trusted":true,"id":"SxO9tgYWGq1I","execution":{"iopub.status.busy":"2025-05-11T16:16:55.105490Z","iopub.execute_input":"2025-05-11T16:16:55.105849Z","iopub.status.idle":"2025-05-11T16:16:55.112702Z","shell.execute_reply.started":"2025-05-11T16:16:55.105820Z","shell.execute_reply":"2025-05-11T16:16:55.111865Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def compute_iou(pred, target, threshold=0.5, eps=1e-6):\n    pred_bin = (pred > threshold).float()\n    target_bin = (target > 0.5).float()\n\n    intersection = (pred_bin * target_bin).sum(dim=(1, 2, 3))\n    union = (pred_bin + target_bin - pred_bin * target_bin).sum(dim=(1, 2, 3))\n    iou = (intersection + eps) / (union + eps)\n    return iou.mean().item()\n\ndef compute_accuracy(pred, target, threshold=0.5):\n    pred_bin = (pred > threshold).float()\n    correct = (pred_bin == target).float()\n    return correct.mean().item()\n","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:16:55.113855Z","iopub.execute_input":"2025-05-11T16:16:55.114150Z","iopub.status.idle":"2025-05-11T16:16:55.129632Z","shell.execute_reply.started":"2025-05-11T16:16:55.114129Z","shell.execute_reply":"2025-05-11T16:16:55.128938Z"},"trusted":true,"id":"5fvcnfGxGq1I"},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# params = {\n#     \"lr\": 10 ** random.uniform(-5, -3)\n# }\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = UNet().to(device)\n# train_fcn_model(params)\n# show_prediction(model, val_loader)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:16:55.130604Z","iopub.execute_input":"2025-05-11T16:16:55.130867Z","iopub.status.idle":"2025-05-11T16:16:55.142731Z","shell.execute_reply.started":"2025-05-11T16:16:55.130841Z","shell.execute_reply":"2025-05-11T16:16:55.141981Z"},"trusted":true,"id":"6AalOtO1Gq1J"},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Training loop","metadata":{"id":"MlaGbfwEGq1N"}},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, epoch, loss, filename=\"checkpoint.pth\"):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n    }\n    torch.save(checkpoint, filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:16:55.143557Z","iopub.execute_input":"2025-05-11T16:16:55.143840Z","iopub.status.idle":"2025-05-11T16:16:55.155244Z","shell.execute_reply.started":"2025-05-11T16:16:55.143818Z","shell.execute_reply":"2025-05-11T16:16:55.154498Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class EarlyStopping:\n    #GeeksforGeeks\n    def __init__(self, patience=5, delta=0):\n        self.patience = patience\n        self.delta = delta\n        self.best_score = None\n        self.early_stop = False\n        self.counter = 0\n        self.best_model_state = None\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.best_model_state = model.state_dict()\n        elif score <= self.best_score + self.delta:\n            self.counter += 1\n            print(\"counter set to \", self.counter)\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.best_model_state = model.state_dict()\n            self.counter = 0\n\n    def load_best_model(self, model):\n        model.load_state_dict(self.best_model_state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:16:55.156021Z","iopub.execute_input":"2025-05-11T16:16:55.156272Z","iopub.status.idle":"2025-05-11T16:16:55.167859Z","shell.execute_reply.started":"2025-05-11T16:16:55.156251Z","shell.execute_reply":"2025-05-11T16:16:55.167102Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def aleatoric_loss(pred_mean, pred_var, target, epsilon=1e-6):\n    loss = 0.5 * torch.log(pred_var + epsilon) + 0.5 * ((target - pred_mean) ** 2) / (pred_var + epsilon)\n    return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:16:55.168630Z","iopub.execute_input":"2025-05-11T16:16:55.169316Z","iopub.status.idle":"2025-05-11T16:16:55.181310Z","shell.execute_reply.started":"2025-05-11T16:16:55.169294Z","shell.execute_reply":"2025-05-11T16:16:55.180559Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"\ndef train_model(model, train_dataset, val_dataset, params, device):\n    \"\"\"\n    A reusable training loop that dynamically creates components based on hyperparameter tuning parameters.\n\n    Args:\n        model (torch.nn.Module): The model to train (e.g., U-Net or Vision Transformer).\n        train_dataset (Dataset): The training dataset.\n        val_dataset (Dataset): The validation dataset.\n        params (dict): Dictionary containing hyperparameters (e.g., learning rate, batch size, optimizer type).\n        device (torch.device): Device to train on (e.g., 'cuda' or 'cpu').\n        num_epochs (int): Number of epochs to train for.\n\n    Returns:\n        dict: A dictionary containing training and validation losses, IoU, and accuracy for each epoch.\n    \"\"\"\n    # === Create DataLoaders ===\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=params[\"batch_size\"], shuffle=True\n    )\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=params[\"batch_size\"], shuffle=False\n    )\n    \n\n    # === Define Optimizer ===\n    if params[\"optimizer\"] == \"adam\":\n        optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n    elif params[\"optimizer\"] == \"adamw\":\n        optimizer = torch.optim.AdamW(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n    elif params[\"optimizer\"] == \"sgd\":\n        optimizer = torch.optim.SGD(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"], momentum=0.9)\n    else:\n        raise ValueError(f\"Unsupported optimizer: {params['optimizer']}\")\n\n    num_epochs = params.get(\"num_epochs\", 10)\n\n    # === Define Loss Function ===\n    pos_weight = params.get(\"pos_weight\", 0.85 / 0.15) # Default to 1.0 if not provided\n\n    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n\n    # === Training Loop ===\n    history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"val_iou\": [],\n        \"val_acc\": []\n    }\n    early_stopping = EarlyStopping(patience=5, delta=0.01)\n\n    for epoch in range(num_epochs):\n        # === Training ===\n        model.train()\n        train_loss = 0.0\n\n        for x_batch, y_batch in train_loader:\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n\n            # Forward pass\n            outputs = model(x_batch)\n            loss = criterion(outputs, y_batch)\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * x_batch.size(0)\n\n        train_loss /= len(train_loader.dataset)\n        history[\"train_loss\"].append(train_loss)\n\n        # === Validation ===\n        model.eval()\n        val_loss = 0.0\n        total_iou = 0.0\n        total_acc = 0.0\n        n_samples = 0\n\n        if epoch % 5 == 0:\n            filename = f\"checkpoint_epoch_{epoch+1}.pth\"\n            save_checkpoint(model, optimizer, epoch, loss, filename)\n\n        with torch.no_grad():\n            for x_val, y_val in val_loader:\n                x_val, y_val = x_val.to(device), y_val.to(device)\n\n                # Forward pass\n                outputs = model(x_val)\n                loss = criterion(outputs, y_val)\n                val_loss += loss.item() * x_val.size(0)\n\n                # Metrics\n                batch_iou = compute_iou(torch.sigmoid(outputs), y_val)\n                batch_acc = compute_accuracy(torch.sigmoid(outputs), y_val)\n\n                total_iou += batch_iou * x_val.size(0)\n                total_acc += batch_acc * x_val.size(0)\n                n_samples += x_val.size(0)\n\n        val_loss /= len(val_loader.dataset)\n        mean_iou = total_iou / n_samples\n        mean_acc = total_acc / n_samples\n\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_iou\"].append(mean_iou)\n        history[\"val_acc\"].append(mean_acc)\n\n        # Print epoch summary\n        print(f\"Epoch {epoch + 1}/{num_epochs} | \"\n              f\"Train Loss: {train_loss:.4f} | \"\n              f\"Val Loss: {val_loss:.4f} | \"\n              f\"IoU: {mean_iou:.4f} | Acc: {mean_acc:.4f}\")\n        early_stopping(val_loss, model)\n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n\n    return history","metadata":{"id":"pchrLaasGq1N","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T21:05:47.917861Z","iopub.execute_input":"2025-05-11T21:05:47.918140Z","iopub.status.idle":"2025-05-11T21:05:47.933272Z","shell.execute_reply.started":"2025-05-11T21:05:47.918120Z","shell.execute_reply":"2025-05-11T21:05:47.932581Z"}},"outputs":[],"execution_count":118},{"cell_type":"markdown","source":"# FCN","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass UNet(nn.Module):\n    def __init__(self, dropout_rate=0.3, in_channels=12, out_channels=1):\n        super(UNet, self).__init__()\n\n        def conv_block(in_c, out_c, dropout_rate):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_c),\n                nn.LeakyReLU(inplace=True),\n                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_c),\n                nn.LeakyReLU(inplace=True),\n                nn.Dropout2d(dropout_rate)\n            )\n\n        self.encoder1 = conv_block(in_channels, 64, dropout_rate)\n        self.encoder2 = conv_block(64, 128, dropout_rate)\n        self.encoder3 = conv_block(128, 256, dropout_rate)\n        self.encoder4 = conv_block(256, 256, dropout_rate)\n        \n        self.pool = nn.MaxPool2d(2)\n\n        self.bottleneck = nn.Sequential(\n            # conv_block(128, 256, dropout_rate),\n            conv_block(256, 512, dropout_rate),\n            conv_block(512, 512, dropout_rate)\n        )\n\n        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.decoder3 = conv_block(512, 256, 0)\n        \n        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.decoder2 = conv_block(256, 128, 0)\n\n        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.decoder1 = conv_block(128, 64, 0)\n\n        self.output_layer = nn.Sequential(\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, out_channels, kernel_size=1)\n        )\n\n\n    def forward(self, x):\n        # Encoder\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(self.pool(e1))\n        e3 = self.encoder3(self.pool(e2))\n        e4 = self.encoder4(self.pool(e3))\n\n        # Bottleneck\n        b = self.bottleneck(self.pool(e3))\n\n        # Decoder\n        d3 = self.up3(b)#+ e3 \n        d3 = torch.cat([d3, e3], dim=1)\n        d3 = self.decoder3(d3)\n\n        d2 = self.up2(d3)#+ \n        d2 = torch.cat([d2, e2], dim=1)\n        d2 = self.decoder2(d2)\n\n        d1 = self.up1(d2)#+ e1\n        d1 = torch.cat([d1, e1], dim=1)\n        d1 = self.decoder1(d1)\n\n        pred_map = self.output_layer(d1)\n        \n        return pred_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:23:45.170332Z","iopub.execute_input":"2025-05-11T17:23:45.170819Z","iopub.status.idle":"2025-05-11T17:23:45.180709Z","shell.execute_reply.started":"2025-05-11T17:23:45.170793Z","shell.execute_reply":"2025-05-11T17:23:45.179891Z"}},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":"# Hyperparameter tuning","metadata":{"id":"nZ4gxdLLGq1N"}},{"cell_type":"code","source":"import random\nfrom itertools import count\nimport gc\n\ndef random_search(model_type, device, n_trials=12):\n    best_val_loss = float('inf')\n    best_params = None\n\n    for trial in range(n_trials):\n        print(\"trial number \", trial+1)\n        if model_type == \"unet\":\n            params = {\n                \"lr\": 10 ** random.uniform(-5, -3),\n                \"batch_size\": random.choice([16, 32, 64]),\n                \"optimizer\": random.choice([\"adam\", \"adamw\", \"sgd\"]),\n                \"weight_decay\": random.choice([0, 1e-4, 1e-3]),\n                \"dropout_rate\": random.uniform(0, 0.5),\n                \"num_epochs\": 10,\n            }\n            print(params)\n            model = UNet(dropout_rate = params[\"dropout_rate\"]).to(device)\n        history = train_model(model, torch_dataset, torch_dataset_val, params, device)\n\n        val_loss = history[\"val_loss\"][-1]\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_params = params\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    return best_params","metadata":{"id":"cnTYC53FGq1O","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:10:41.959265Z","iopub.execute_input":"2025-05-11T17:10:41.959765Z","iopub.status.idle":"2025-05-11T17:10:41.965768Z","shell.execute_reply.started":"2025-05-11T17:10:41.959740Z","shell.execute_reply":"2025-05-11T17:10:41.964927Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nbest_params_unet = random_search(model_type = \"unet\", device = device)\nprint(best_params_unet)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1nE853BMBlZ","outputId":"915900bb-6cbd-4e87-808c-e5541d4ddc6f","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:23:47.899872Z","iopub.execute_input":"2025-05-11T17:23:47.900141Z","iopub.status.idle":"2025-05-11T18:34:09.764722Z","shell.execute_reply.started":"2025-05-11T17:23:47.900122Z","shell.execute_reply":"2025-05-11T18:34:09.764083Z"}},"outputs":[{"name":"stdout","text":"cuda\ntrial number  1\n{'lr': 0.00040188025079528457, 'batch_size': 64, 'optimizer': 'sgd', 'weight_decay': 0.001, 'dropout_rate': 0.31946242215179077, 'num_epochs': 10}\nEpoch 1/10 | Train Loss: 0.4016 | Val Loss: 0.3030 | IoU: 0.0649 | Acc: 0.9846\nEpoch 2/10 | Train Loss: 0.2387 | Val Loss: 0.2757 | IoU: 0.0649 | Acc: 0.9846\nEpoch 3/10 | Train Loss: 0.2250 | Val Loss: 0.2713 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  1\nEpoch 4/10 | Train Loss: 0.2149 | Val Loss: 0.2645 | IoU: 0.0649 | Acc: 0.9846\nEpoch 5/10 | Train Loss: 0.2036 | Val Loss: 0.2648 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  1\nEpoch 6/10 | Train Loss: 0.1924 | Val Loss: 0.2604 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  2\nEpoch 7/10 | Train Loss: 0.1800 | Val Loss: 0.2533 | IoU: 0.1043 | Acc: 0.9820\nEpoch 8/10 | Train Loss: 0.1705 | Val Loss: 0.2522 | IoU: 0.1415 | Acc: 0.9796\ncounter set to  1\nEpoch 9/10 | Train Loss: 0.1633 | Val Loss: 0.2481 | IoU: 0.1530 | Acc: 0.9773\ncounter set to  2\nEpoch 10/10 | Train Loss: 0.1577 | Val Loss: 0.2454 | IoU: 0.1566 | Acc: 0.9757\ncounter set to  3\ntrial number  2\n{'lr': 0.0002812756953877845, 'batch_size': 64, 'optimizer': 'adamw', 'weight_decay': 0, 'dropout_rate': 0.40234294604136445, 'num_epochs': 10}\nEpoch 1/10 | Train Loss: 0.1974 | Val Loss: 0.1923 | IoU: 0.1921 | Acc: 0.9697\nEpoch 2/10 | Train Loss: 0.1177 | Val Loss: 0.1835 | IoU: 0.2136 | Acc: 0.9728\ncounter set to  1\nEpoch 3/10 | Train Loss: 0.1123 | Val Loss: 0.1806 | IoU: 0.2241 | Acc: 0.9734\nEpoch 4/10 | Train Loss: 0.1092 | Val Loss: 0.1795 | IoU: 0.2297 | Acc: 0.9754\ncounter set to  1\nEpoch 5/10 | Train Loss: 0.1079 | Val Loss: 0.1744 | IoU: 0.2199 | Acc: 0.9738\ncounter set to  2\nEpoch 6/10 | Train Loss: 0.1059 | Val Loss: 0.1762 | IoU: 0.2366 | Acc: 0.9753\ncounter set to  3\nEpoch 7/10 | Train Loss: 0.1060 | Val Loss: 0.1868 | IoU: 0.2327 | Acc: 0.9755\ncounter set to  4\nEpoch 8/10 | Train Loss: 0.1033 | Val Loss: 0.1842 | IoU: 0.2289 | Acc: 0.9735\ncounter set to  5\nEarly stopping\ntrial number  3\n{'lr': 2.3423833094103366e-05, 'batch_size': 16, 'optimizer': 'adam', 'weight_decay': 0.0001, 'dropout_rate': 0.4465267982200054, 'num_epochs': 10}\nEpoch 1/10 | Train Loss: 0.2780 | Val Loss: 0.2227 | IoU: 0.1965 | Acc: 0.9800\nEpoch 2/10 | Train Loss: 0.1365 | Val Loss: 0.2102 | IoU: 0.2121 | Acc: 0.9721\nEpoch 3/10 | Train Loss: 0.1225 | Val Loss: 0.2025 | IoU: 0.2194 | Acc: 0.9743\ncounter set to  1\nEpoch 4/10 | Train Loss: 0.1174 | Val Loss: 0.2052 | IoU: 0.2252 | Acc: 0.9739\ncounter set to  2\nEpoch 5/10 | Train Loss: 0.1145 | Val Loss: 0.2068 | IoU: 0.2223 | Acc: 0.9709\ncounter set to  3\nEpoch 6/10 | Train Loss: 0.1119 | Val Loss: 0.1915 | IoU: 0.2234 | Acc: 0.9713\nEpoch 7/10 | Train Loss: 0.1101 | Val Loss: 0.1933 | IoU: 0.2355 | Acc: 0.9732\ncounter set to  1\nEpoch 8/10 | Train Loss: 0.1084 | Val Loss: 0.1796 | IoU: 0.2304 | Acc: 0.9721\nEpoch 9/10 | Train Loss: 0.1073 | Val Loss: 0.1826 | IoU: 0.2304 | Acc: 0.9707\ncounter set to  1\nEpoch 10/10 | Train Loss: 0.1060 | Val Loss: 0.1793 | IoU: 0.2370 | Acc: 0.9711\ncounter set to  2\ntrial number  4\n{'lr': 3.223666378745681e-05, 'batch_size': 16, 'optimizer': 'sgd', 'weight_decay': 0.001, 'dropout_rate': 0.43055226660372675, 'num_epochs': 10}\nEpoch 1/10 | Train Loss: 0.5727 | Val Loss: 0.5154 | IoU: 0.0649 | Acc: 0.9846\nEpoch 2/10 | Train Loss: 0.3691 | Val Loss: 0.3772 | IoU: 0.0649 | Acc: 0.9846\nEpoch 3/10 | Train Loss: 0.2811 | Val Loss: 0.3283 | IoU: 0.0649 | Acc: 0.9846\nEpoch 4/10 | Train Loss: 0.2526 | Val Loss: 0.3055 | IoU: 0.0649 | Acc: 0.9846\nEpoch 5/10 | Train Loss: 0.2425 | Val Loss: 0.2962 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  1\nEpoch 6/10 | Train Loss: 0.2370 | Val Loss: 0.2901 | IoU: 0.0649 | Acc: 0.9846\nEpoch 7/10 | Train Loss: 0.2338 | Val Loss: 0.2872 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  1\nEpoch 8/10 | Train Loss: 0.2314 | Val Loss: 0.2855 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  2\nEpoch 9/10 | Train Loss: 0.2288 | Val Loss: 0.2840 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  3\nEpoch 10/10 | Train Loss: 0.2262 | Val Loss: 0.2816 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  4\ntrial number  5\n{'lr': 8.587651694090642e-05, 'batch_size': 32, 'optimizer': 'sgd', 'weight_decay': 0.001, 'dropout_rate': 0.46810698075798635, 'num_epochs': 10}\nEpoch 1/10 | Train Loss: 0.6306 | Val Loss: 0.5306 | IoU: 0.0649 | Acc: 0.9846\nEpoch 2/10 | Train Loss: 0.3672 | Val Loss: 0.3552 | IoU: 0.0649 | Acc: 0.9846\nEpoch 3/10 | Train Loss: 0.2676 | Val Loss: 0.3074 | IoU: 0.0649 | Acc: 0.9846\nEpoch 4/10 | Train Loss: 0.2448 | Val Loss: 0.2915 | IoU: 0.0649 | Acc: 0.9846\nEpoch 5/10 | Train Loss: 0.2371 | Val Loss: 0.2895 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  1\nEpoch 6/10 | Train Loss: 0.2328 | Val Loss: 0.2870 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  2\nEpoch 7/10 | Train Loss: 0.2297 | Val Loss: 0.2815 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  3\nEpoch 8/10 | Train Loss: 0.2270 | Val Loss: 0.2810 | IoU: 0.0649 | Acc: 0.9846\nEpoch 9/10 | Train Loss: 0.2241 | Val Loss: 0.2781 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  1\nEpoch 10/10 | Train Loss: 0.2219 | Val Loss: 0.2749 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  2\ntrial number  6\n{'lr': 0.00012098280722819225, 'batch_size': 64, 'optimizer': 'adam', 'weight_decay': 0.0001, 'dropout_rate': 0.27039140472529627, 'num_epochs': 10}\nEpoch 1/10 | Train Loss: 0.2551 | Val Loss: 0.2023 | IoU: 0.2158 | Acc: 0.9744\nEpoch 2/10 | Train Loss: 0.1203 | Val Loss: 0.1806 | IoU: 0.1982 | Acc: 0.9700\nEpoch 3/10 | Train Loss: 0.1125 | Val Loss: 0.1773 | IoU: 0.2212 | Acc: 0.9717\ncounter set to  1\nEpoch 4/10 | Train Loss: 0.1091 | Val Loss: 0.1975 | IoU: 0.2266 | Acc: 0.9752\ncounter set to  2\nEpoch 5/10 | Train Loss: 0.1064 | Val Loss: 0.1878 | IoU: 0.2270 | Acc: 0.9718\ncounter set to  3\nEpoch 6/10 | Train Loss: 0.1044 | Val Loss: 0.1734 | IoU: 0.2357 | Acc: 0.9741\ncounter set to  4\nEpoch 7/10 | Train Loss: 0.1033 | Val Loss: 0.2009 | IoU: 0.2414 | Acc: 0.9767\ncounter set to  5\nEarly stopping\ntrial number  7\n{'lr': 0.0004743102816657763, 'batch_size': 64, 'optimizer': 'adamw', 'weight_decay': 0, 'dropout_rate': 0.19050225456894704, 'num_epochs': 10}\nEpoch 1/10 | Train Loss: 0.1566 | Val Loss: 0.1998 | IoU: 0.1933 | Acc: 0.9700\nEpoch 2/10 | Train Loss: 0.1137 | Val Loss: 0.1926 | IoU: 0.2181 | Acc: 0.9739\ncounter set to  1\nEpoch 3/10 | Train Loss: 0.1089 | Val Loss: 0.1821 | IoU: 0.2094 | Acc: 0.9711\nEpoch 4/10 | Train Loss: 0.1063 | Val Loss: 0.1755 | IoU: 0.2233 | Acc: 0.9731\ncounter set to  1\nEpoch 5/10 | Train Loss: 0.1036 | Val Loss: 0.1877 | IoU: 0.2259 | Acc: 0.9729\ncounter set to  2\nEpoch 6/10 | Train Loss: 0.1029 | Val Loss: 0.1951 | IoU: 0.2405 | Acc: 0.9794\ncounter set to  3\nEpoch 7/10 | Train Loss: 0.1011 | Val Loss: 0.1894 | IoU: 0.2321 | Acc: 0.9768\ncounter set to  4\nEpoch 8/10 | Train Loss: 0.1003 | Val Loss: 0.1785 | IoU: 0.2317 | Acc: 0.9771\ncounter set to  5\nEarly stopping\ntrial number  8\n{'lr': 0.0006341123757912465, 'batch_size': 64, 'optimizer': 'adamw', 'weight_decay': 0.0001, 'dropout_rate': 0.49673056652896724, 'num_epochs': 10}\nEpoch 1/10 | Train Loss: 0.1740 | Val Loss: 0.2015 | IoU: 0.1875 | Acc: 0.9652\nEpoch 2/10 | Train Loss: 0.1177 | Val Loss: 0.1847 | IoU: 0.2141 | Acc: 0.9701\nEpoch 3/10 | Train Loss: 0.1136 | Val Loss: 0.1744 | IoU: 0.2182 | Acc: 0.9715\nEpoch 4/10 | Train Loss: 0.1115 | Val Loss: 0.1792 | IoU: 0.2290 | Acc: 0.9737\ncounter set to  1\nEpoch 5/10 | Train Loss: 0.1092 | Val Loss: 0.1856 | IoU: 0.2071 | Acc: 0.9680\ncounter set to  2\nEpoch 6/10 | Train Loss: 0.1075 | Val Loss: 0.1928 | IoU: 0.2261 | Acc: 0.9736\ncounter set to  3\nEpoch 7/10 | Train Loss: 0.1070 | Val Loss: 0.1793 | IoU: 0.2322 | Acc: 0.9711\ncounter set to  4\nEpoch 8/10 | Train Loss: 0.1070 | Val Loss: 0.1830 | IoU: 0.2293 | Acc: 0.9729\ncounter set to  5\nEarly stopping\ntrial number  9\n{'lr': 0.0001717637477807232, 'batch_size': 16, 'optimizer': 'adamw', 'weight_decay': 0.0001, 'dropout_rate': 0.17553054859474343, 'num_epochs': 10}\nEpoch 1/10 | Train Loss: 0.1391 | Val Loss: 0.1758 | IoU: 0.2278 | Acc: 0.9743\nEpoch 2/10 | Train Loss: 0.1113 | Val Loss: 0.1689 | IoU: 0.2284 | Acc: 0.9723\ncounter set to  1\nEpoch 3/10 | Train Loss: 0.1078 | Val Loss: 0.1743 | IoU: 0.2315 | Acc: 0.9720\ncounter set to  2\nEpoch 4/10 | Train Loss: 0.1061 | Val Loss: 0.1805 | IoU: 0.2385 | Acc: 0.9737\ncounter set to  3\nEpoch 5/10 | Train Loss: 0.1027 | Val Loss: 0.1958 | IoU: 0.2357 | Acc: 0.9773\ncounter set to  4\nEpoch 6/10 | Train Loss: 0.1023 | Val Loss: 0.1796 | IoU: 0.2370 | Acc: 0.9738\ncounter set to  5\nEarly stopping\ntrial number  10\n{'lr': 2.0679686707192324e-05, 'batch_size': 16, 'optimizer': 'adamw', 'weight_decay': 0.001, 'dropout_rate': 0.39887750530776844, 'num_epochs': 10}\nEpoch 1/10 | Train Loss: 0.3230 | Val Loss: 0.2161 | IoU: 0.1915 | Acc: 0.9762\nEpoch 2/10 | Train Loss: 0.1386 | Val Loss: 0.1970 | IoU: 0.2170 | Acc: 0.9732\nEpoch 3/10 | Train Loss: 0.1222 | Val Loss: 0.2045 | IoU: 0.2211 | Acc: 0.9732\ncounter set to  1\nEpoch 4/10 | Train Loss: 0.1167 | Val Loss: 0.1927 | IoU: 0.2210 | Acc: 0.9730\ncounter set to  2\nEpoch 5/10 | Train Loss: 0.1141 | Val Loss: 0.1944 | IoU: 0.2234 | Acc: 0.9724\ncounter set to  3\nEpoch 6/10 | Train Loss: 0.1112 | Val Loss: 0.1919 | IoU: 0.2267 | Acc: 0.9727\ncounter set to  4\nEpoch 7/10 | Train Loss: 0.1093 | Val Loss: 0.1869 | IoU: 0.2228 | Acc: 0.9694\nEpoch 8/10 | Train Loss: 0.1086 | Val Loss: 0.1768 | IoU: 0.2335 | Acc: 0.9711\nEpoch 9/10 | Train Loss: 0.1071 | Val Loss: 0.1950 | IoU: 0.2343 | Acc: 0.9733\ncounter set to  1\nEpoch 10/10 | Train Loss: 0.1059 | Val Loss: 0.1879 | IoU: 0.2428 | Acc: 0.9752\ncounter set to  2\ntrial number  11\n{'lr': 2.119611040156347e-05, 'batch_size': 64, 'optimizer': 'sgd', 'weight_decay': 0.0001, 'dropout_rate': 0.18769783811639107, 'num_epochs': 10}\nEpoch 1/10 | Train Loss: 0.7080 | Val Loss: 0.6822 | IoU: 0.0080 | Acc: 0.9425\nEpoch 2/10 | Train Loss: 0.6466 | Val Loss: 0.6353 | IoU: 0.0349 | Acc: 0.9827\nEpoch 3/10 | Train Loss: 0.5954 | Val Loss: 0.5961 | IoU: 0.0619 | Acc: 0.9845\nEpoch 4/10 | Train Loss: 0.5499 | Val Loss: 0.5582 | IoU: 0.0642 | Acc: 0.9846\nEpoch 5/10 | Train Loss: 0.5086 | Val Loss: 0.5244 | IoU: 0.0649 | Acc: 0.9846\nEpoch 6/10 | Train Loss: 0.4713 | Val Loss: 0.4936 | IoU: 0.0649 | Acc: 0.9846\nEpoch 7/10 | Train Loss: 0.4367 | Val Loss: 0.4611 | IoU: 0.0649 | Acc: 0.9846\nEpoch 8/10 | Train Loss: 0.4060 | Val Loss: 0.4319 | IoU: 0.0649 | Acc: 0.9846\nEpoch 9/10 | Train Loss: 0.3789 | Val Loss: 0.4109 | IoU: 0.0649 | Acc: 0.9846\nEpoch 10/10 | Train Loss: 0.3555 | Val Loss: 0.3919 | IoU: 0.0649 | Acc: 0.9846\ntrial number  12\n{'lr': 7.73724845637636e-05, 'batch_size': 32, 'optimizer': 'sgd', 'weight_decay': 0.0001, 'dropout_rate': 0.24998301299915204, 'num_epochs': 10}\nEpoch 1/10 | Train Loss: 0.4773 | Val Loss: 0.3884 | IoU: 0.0649 | Acc: 0.9846\nEpoch 2/10 | Train Loss: 0.2785 | Val Loss: 0.3099 | IoU: 0.0649 | Acc: 0.9846\nEpoch 3/10 | Train Loss: 0.2458 | Val Loss: 0.2934 | IoU: 0.0649 | Acc: 0.9846\nEpoch 4/10 | Train Loss: 0.2360 | Val Loss: 0.2883 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  1\nEpoch 5/10 | Train Loss: 0.2306 | Val Loss: 0.2860 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  2\nEpoch 6/10 | Train Loss: 0.2260 | Val Loss: 0.2838 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  3\nEpoch 7/10 | Train Loss: 0.2221 | Val Loss: 0.2816 | IoU: 0.0649 | Acc: 0.9846\nEpoch 8/10 | Train Loss: 0.2182 | Val Loss: 0.2810 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  1\nEpoch 9/10 | Train Loss: 0.2139 | Val Loss: 0.2779 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  2\nEpoch 10/10 | Train Loss: 0.2096 | Val Loss: 0.2775 | IoU: 0.0649 | Acc: 0.9846\ncounter set to  3\n{'lr': 0.0004743102816657763, 'batch_size': 64, 'optimizer': 'adamw', 'weight_decay': 0, 'dropout_rate': 0.19050225456894704, 'num_epochs': 10}\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"# Next steps:\n# 1. Run hyperparameter tuning on 8 epochs for 8 trials +\n# 2. Train both models for 20 epochs with early stopping + \n# 3. Visualize attention maps\n# 4. Visualize activations + \n# 5. Identify misclassified samples \n# 6. Test performance with merging the \"uncertainty\" class into fire/no-fire\n\n# _____NOTES___________\n# skip connections invrease losses,sccuracy but not IoU\n# make sure to change forward when removing skip connctions\n# don't go over 512\n\n","metadata":{"id":"e2ylGW19UchS","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:18:45.721304Z","iopub.status.idle":"2025-05-11T16:18:45.721540Z","shell.execute_reply.started":"2025-05-11T16:18:45.721437Z","shell.execute_reply":"2025-05-11T16:18:45.721447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train with tuned parameters\nunet = UNet().to(device)\n# manual_params = {'lr': 4.511683720714809e-05, \n#                   'batch_size': 64, \n#                   'optimizer': 'adamw', \n#                   'weight_decay': 0.001, \n#                   'num_epochs': 5}\nbest_params_unet['num_epochs'] = 50\nprint(best_params_unet)\nunet_history = train_model(unet, torch_dataset, torch_dataset_val, best_params_unet, device)\nunet.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:34:09.765783Z","iopub.execute_input":"2025-05-11T18:34:09.766030Z","iopub.status.idle":"2025-05-11T18:39:17.899800Z","shell.execute_reply.started":"2025-05-11T18:34:09.766012Z","shell.execute_reply":"2025-05-11T18:39:17.899137Z"}},"outputs":[{"name":"stdout","text":"{'lr': 0.0004743102816657763, 'batch_size': 64, 'optimizer': 'adamw', 'weight_decay': 0, 'dropout_rate': 0.19050225456894704, 'num_epochs': 50}\nEpoch 1/50 | Train Loss: 0.1641 | Val Loss: 0.1901 | IoU: 0.2003 | Acc: 0.9666\nEpoch 2/50 | Train Loss: 0.1157 | Val Loss: 0.1845 | IoU: 0.2121 | Acc: 0.9705\ncounter set to  1\nEpoch 3/50 | Train Loss: 0.1103 | Val Loss: 0.1730 | IoU: 0.2266 | Acc: 0.9733\nEpoch 4/50 | Train Loss: 0.1083 | Val Loss: 0.1717 | IoU: 0.2166 | Acc: 0.9709\ncounter set to  1\nEpoch 5/50 | Train Loss: 0.1066 | Val Loss: 0.1902 | IoU: 0.2276 | Acc: 0.9741\ncounter set to  2\nEpoch 6/50 | Train Loss: 0.1046 | Val Loss: 0.1794 | IoU: 0.2398 | Acc: 0.9752\ncounter set to  3\nEpoch 7/50 | Train Loss: 0.1041 | Val Loss: 0.1773 | IoU: 0.2385 | Acc: 0.9735\ncounter set to  4\nEpoch 8/50 | Train Loss: 0.1020 | Val Loss: 0.1723 | IoU: 0.2328 | Acc: 0.9727\ncounter set to  5\nEarly stopping\n","output_type":"stream"},{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"UNet(\n  (encoder1): Sequential(\n    (0): Conv2d(12, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    (6): Dropout2d(p=0.3, inplace=False)\n  )\n  (encoder2): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    (6): Dropout2d(p=0.3, inplace=False)\n  )\n  (encoder3): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    (6): Dropout2d(p=0.3, inplace=False)\n  )\n  (encoder4): Sequential(\n    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    (6): Dropout2d(p=0.3, inplace=False)\n  )\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (bottleneck): Sequential(\n    (0): Sequential(\n      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n      (6): Dropout2d(p=0.3, inplace=False)\n    )\n    (1): Sequential(\n      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n      (6): Dropout2d(p=0.3, inplace=False)\n    )\n  )\n  (up3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n  (decoder3): Sequential(\n    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    (6): Dropout2d(p=0, inplace=False)\n  )\n  (up2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n  (decoder2): Sequential(\n    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    (6): Dropout2d(p=0, inplace=False)\n  )\n  (up1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n  (decoder1): Sequential(\n    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    (6): Dropout2d(p=0, inplace=False)\n  )\n  (output_layer): Sequential(\n    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n  )\n)"},"metadata":{}}],"execution_count":78},{"cell_type":"markdown","source":"# Generate Saliency maps","metadata":{}},{"cell_type":"code","source":"from torchcam.methods import SmoothGradCAMpp\n\ntarget_layer = 'encoder2' \ncam_extractor = SmoothGradCAMpp(unet, target_layer)\n\n# Get one input from the validation set\nx_val, y_val = next(iter(val_loader))\nx_val, y_val = x_val.to(device), y_val.to(device)\n\n# Select the first sample in the batch\ninput_sample = x_val[0].unsqueeze(0)  # Shape: (1, 12, 64, 64)\nground_truth = y_val[0].unsqueeze(0)  # Shape: (1, 1, 64, 64)\npred_mask = unet(x_val)\n\n\n# Generate saliency maps for each input channel\nsaliency_maps = []\n\nfor channel_idx in range(input_sample.shape[1]):\n    # Zero out all other channels except the current one\n    single_channel_input = torch.zeros_like(input_sample)\n    single_channel_input[:, channel_idx, :, :] = input_sample[:, channel_idx, :, :]\n\n    # Forward pass and generate CAM\n    output = unet(single_channel_input)\n\n    activation_map = cam_extractor(0, output) # Class index for fire mask\n    saliency_maps.append(activation_map[0].cpu().numpy())\n\nplt.figure(figsize=(18, 12))\n    \n# Plot input channels (first 12)\nplt.subplot(3, 4, 1)\nplt.imshow(input_sample[0, 0].cpu().numpy(), cmap='gray')\nplt.title(f\"Input: {feature_names[0]}\")\nplt.axis('off')\n\nfor i in range(1, min(12, input_sample.shape[1])):\n    plt.subplot(3, 4, i+1)\n    plt.imshow(input_sample[0, i].cpu().numpy(), cmap='gray')\n    plt.title(f\"Input: {feature_names[i]}\")\n    plt.axis('off')\n\n# Plot ground truth and prediction\nplt.subplot(3, 4, 11)\nplt.imshow(ground_truth.cpu().numpy().squeeze(), cmap='hot', vmin=0, vmax=1)\nplt.title(\"Ground Truth Fire Mask\")\nplt.axis('off')\n\nplt.subplot(3, 4, 12)\nplt.imshow(pred_mask[0].cpu().detach().numpy().squeeze(), cmap='hot', vmin=0, vmax=1)\nplt.title(\"Predicted Fire Mask\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Plot saliency maps for each feature\nfeature_names = INPUT_FEATURES  # Use the feature names defined earlier\nfig, axes = plt.subplots(3, 4, figsize=(15, 10))\nfor idx, ax in enumerate(axes.flat):\n    if idx < len(feature_names):\n        if saliency_maps[idx].ndim > 2:\n            saliency_maps[idx] = saliency_maps[idx].squeeze() \n        ax.imshow(saliency_maps[idx], cmap='jet', interpolation='nearest')\n        ax.set_title(feature_names[idx])\n        ax.axis('off')\nplt.tight_layout()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:03:18.790162Z","iopub.execute_input":"2025-05-12T01:03:18.790427Z","iopub.status.idle":"2025-05-12T01:03:18.803836Z","shell.execute_reply.started":"2025-05-12T01:03:18.790407Z","shell.execute_reply":"2025-05-12T01:03:18.802977Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/198871554.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSmoothGradCAMpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'encoder2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcam_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSmoothGradCAMpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchcam'"],"ename":"ModuleNotFoundError","evalue":"No module named 'torchcam'","output_type":"error"}],"execution_count":3},{"cell_type":"markdown","source":"# Identify misclassified examples","metadata":{}},{"cell_type":"markdown","source":"# Merge uncertainty into fire/non fire examples","metadata":{}}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2824184,"sourceType":"datasetVersion","datasetId":1726926}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport re\nfrom typing import Dict, List, Optional, Text, Tuple\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n\nimport tensorflow as tf\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:47.378134Z","iopub.execute_input":"2025-05-08T03:42:47.378845Z","iopub.status.idle":"2025-05-08T03:42:47.382851Z","shell.execute_reply.started":"2025-05-08T03:42:47.378820Z","shell.execute_reply":"2025-05-08T03:42:47.382183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# load data","metadata":{}},{"cell_type":"code","source":"\"\"\"Constants for the data reader.\"\"\"\n\nINPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph', \n                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n\nOUTPUT_FEATURES = ['FireMask', ]\n\n# Data statistics \n# For each variable, the statistics are ordered in the form:\n# (min_clip, max_clip, mean, standard deviation)\nDATA_STATS = {\n    # Elevation in m.\n    # 0.1 percentile, 99.9 percentile\n    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n    # Pressure\n    # 0.1 percentile, 99.9 percentile\n    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),  # min, max\n    # Precipitation in mm.\n    # Negative values do not make sense, so min is set to 0.\n    # 0., 99.9 percentile\n    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n    # Specific humidity.\n    # Negative values do not make sense, so min is set to 0.\n    # The range of specific humidity is up to 100% so max is 1.\n    'sph': (0., 1., 0.0071658953, 0.0042835088),\n    # Wind direction in degrees clockwise from north.\n    # Thus min set to 0 and max set to 360.\n    'th': (0., 360.0, 190.32976, 72.59854),\n    # Min/max temperature in Kelvin.\n    # -20 degree C, 99.9 percentile\n    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n    # -20 degree C, 99.9 percentile\n    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n    # Wind speed in m/s.\n    # Negative values do not make sense, given there is a wind direction.\n    # 0., 99.9 percentile\n    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n    # NFDRS fire danger index energy release component expressed in BTU's per\n    # square foot.\n    # Negative values do not make sense. Thus min set to zero.\n    # 0., 99.9 percentile\n    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n    # Population density\n    # min, 99.9 percentile\n    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n    # We don't want to normalize the FireMasks.\n    # 1 indicates fire, 0 no fire, -1 unlabeled data\n    'PrevFireMask': (-1., 1., 0., 1.),\n    'FireMask': (-1., 1., 0., 1.)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:47.384211Z","iopub.execute_input":"2025-05-08T03:42:47.384598Z","iopub.status.idle":"2025-05-08T03:42:47.399055Z","shell.execute_reply.started":"2025-05-08T03:42:47.384580Z","shell.execute_reply":"2025-05-08T03:42:47.398486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"Library of common functions used in deep learning neural networks.\n\"\"\"\ndef random_crop_input_and_output_images(\n    input_img: tf.Tensor,\n    output_img: tf.Tensor,\n    sample_size: int,\n    num_in_channels: int,\n    num_out_channels: int,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n  \"\"\"Randomly axis-align crop input and output image tensors.\n\n  Args:\n    input_img: tensor with dimensions HWC.\n    output_img: tensor with dimensions HWC.\n    sample_size: side length (square) to crop to.\n    num_in_channels: number of channels in input_img.\n    num_out_channels: number of channels in output_img.\n  Returns:\n    input_img: tensor with dimensions HWC.\n    output_img: tensor with dimensions HWC.\n  \"\"\"\n  combined = tf.concat([input_img, output_img], axis=2)\n  combined = tf.image.random_crop(\n      combined,\n      [sample_size, sample_size, num_in_channels + num_out_channels])\n  input_img = combined[:, :, 0:num_in_channels]\n  output_img = combined[:, :, -num_out_channels:]\n  return input_img, output_img\n\n\ndef center_crop_input_and_output_images(\n    input_img: tf.Tensor,\n    output_img: tf.Tensor,\n    sample_size: int,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n  \"\"\"Center crops input and output image tensors.\n\n  Args:\n    input_img: tensor with dimensions HWC.\n    output_img: tensor with dimensions HWC.\n    sample_size: side length (square) to crop to.\n  Returns:\n    input_img: tensor with dimensions HWC.\n    output_img: tensor with dimensions HWC.\n  \"\"\"\n  central_fraction = sample_size / input_img.shape[0]\n  input_img = tf.image.central_crop(input_img, central_fraction)\n  output_img = tf.image.central_crop(output_img, central_fraction)\n  return input_img, output_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:47.399670Z","iopub.execute_input":"2025-05-08T03:42:47.399868Z","iopub.status.idle":"2025-05-08T03:42:47.412637Z","shell.execute_reply.started":"2025-05-08T03:42:47.399854Z","shell.execute_reply":"2025-05-08T03:42:47.411933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"Dataset reader for Earth Engine data.\"\"\"\n\ndef _get_base_key(key: Text) -> Text:\n  \"\"\"Extracts the base key from the provided key.\n\n  Earth Engine exports TFRecords containing each data variable with its\n  corresponding variable name. In the case of time sequences, the name of the\n  data variable is of the form 'variable_1', 'variable_2', ..., 'variable_n',\n  where 'variable' is the name of the variable, and n the number of elements\n  in the time sequence. Extracting the base key ensures that each step of the\n  time sequence goes through the same normalization steps.\n  The base key obeys the following naming pattern: '([a-zA-Z]+)'\n  For instance, for an input key 'variable_1', this function returns 'variable'.\n  For an input key 'variable', this function simply returns 'variable'.\n\n  Args:\n    key: Input key.\n\n  Returns:\n    The corresponding base key.\n\n  Raises:\n    ValueError when `key` does not match the expected pattern.\n  \"\"\"\n  match = re.match(r'([a-zA-Z]+)', key)\n  if match:\n    return match.group(1)\n  raise ValueError(\n      'The provided key does not match the expected pattern: {}'.format(key))\n\n\ndef _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n  \"\"\"Clips and rescales inputs with the stats corresponding to `key`.\n\n  Args:\n    inputs: Inputs to clip and rescale.\n    key: Key describing the inputs.\n\n  Returns:\n    Clipped and rescaled input.\n\n  Raises:\n    ValueError if there are no data statistics available for `key`.\n  \"\"\"\n  base_key = _get_base_key(key)\n  if base_key not in DATA_STATS:\n    raise ValueError(\n        'No data statistics available for the requested key: {}.'.format(key))\n  min_val, max_val, _, _ = DATA_STATS[base_key]\n  inputs = tf.clip_by_value(inputs, min_val, max_val)\n  return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n\n\ndef _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n  \"\"\"Clips and normalizes inputs with the stats corresponding to `key`.\n\n  Args:\n    inputs: Inputs to clip and normalize.\n    key: Key describing the inputs.\n\n  Returns:\n    Clipped and normalized input.\n\n  Raises:\n    ValueError if there are no data statistics available for `key`.\n  \"\"\"\n  base_key = _get_base_key(key)\n  if base_key not in DATA_STATS:\n    raise ValueError(\n        'No data statistics available for the requested key: {}.'.format(key))\n  min_val, max_val, mean, std = DATA_STATS[base_key]\n  inputs = tf.clip_by_value(inputs, min_val, max_val)\n  inputs = inputs - mean\n  return tf.math.divide_no_nan(inputs, std)\n\ndef _get_features_dict(\n    sample_size: int,\n    features: List[Text],\n) -> Dict[Text, tf.io.FixedLenFeature]:\n  \"\"\"Creates a features dictionary for TensorFlow IO.\n\n  Args:\n    sample_size: Size of the input tiles (square).\n    features: List of feature names.\n\n  Returns:\n    A features dictionary for TensorFlow IO.\n  \"\"\"\n  sample_shape = [sample_size, sample_size]\n  features = set(features)\n  columns = [\n      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n      for _ in features\n  ]\n  return dict(zip(features, columns))\n\n\ndef _parse_fn(\n    example_proto: tf.train.Example, data_size: int, sample_size: int,\n    num_in_channels: int, clip_and_normalize: bool,\n    clip_and_rescale: bool, random_crop: bool, center_crop: bool,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n  \"\"\"Reads a serialized example.\n\n  Args:\n    example_proto: A TensorFlow example protobuf.\n    data_size: Size of tiles (square) as read from input files.\n    sample_size: Size the tiles (square) when input into the model.\n    num_in_channels: Number of input channels.\n    clip_and_normalize: True if the data should be clipped and normalized.\n    clip_and_rescale: True if the data should be clipped and rescaled.\n    random_crop: True if the data should be randomly cropped.\n    center_crop: True if the data should be cropped in the center.\n\n  Returns:\n    (input_img, output_img) tuple of inputs and outputs to the ML model.\n  \"\"\"\n  if (random_crop and center_crop):\n    raise ValueError('Cannot have both random_crop and center_crop be True')\n  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n  feature_names = input_features + output_features\n  features_dict = _get_features_dict(data_size, feature_names)\n  features = tf.io.parse_single_example(example_proto, features_dict)\n\n  if clip_and_normalize:\n    inputs_list = [\n        _clip_and_normalize(features.get(key), key) for key in input_features\n    ]\n  elif clip_and_rescale:\n    inputs_list = [\n        _clip_and_rescale(features.get(key), key) for key in input_features\n    ]\n  else:\n    inputs_list = [features.get(key) for key in input_features]\n  \n  inputs_stacked = tf.stack(inputs_list, axis=0)\n  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n\n  outputs_list = [features.get(key) for key in output_features]\n  assert outputs_list, 'outputs_list should not be empty'\n  outputs_stacked = tf.stack(outputs_list, axis=0)\n\n  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n                                            'but dimensions of outputs_stacked'\n                                            f' are {outputs_stacked_shape}')\n  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n\n  if random_crop:\n    input_img, output_img = random_crop_input_and_output_images(\n        input_img, output_img, sample_size, num_in_channels, 1)\n  if center_crop:\n    input_img, output_img = center_crop_input_and_output_images(\n        input_img, output_img, sample_size)\n  return input_img, output_img\n\n\ndef get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n                batch_size: int, num_in_channels: int, compression_type: Text,\n                clip_and_normalize: bool, clip_and_rescale: bool,\n                random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n  \"\"\"Gets the dataset from the file pattern.\n\n  Args:\n    file_pattern: Input file pattern.\n    data_size: Size of tiles (square) as read from input files.\n    sample_size: Size the tiles (square) when input into the model.\n    batch_size: Batch size.\n    num_in_channels: Number of input channels.\n    compression_type: Type of compression used for the input files.\n    clip_and_normalize: True if the data should be clipped and normalized, False\n      otherwise.\n    clip_and_rescale: True if the data should be clipped and rescaled, False\n      otherwise.\n    random_crop: True if the data should be randomly cropped.\n    center_crop: True if the data shoulde be cropped in the center.\n\n  Returns:\n    A TensorFlow dataset loaded from the input file pattern, with features\n    described in the constants, and with the shapes determined from the input\n    parameters to this function.\n  \"\"\"\n  if (clip_and_normalize and clip_and_rescale):\n    raise ValueError('Cannot have both normalize and rescale.')\n  dataset = tf.data.Dataset.list_files(file_pattern)\n  dataset = dataset.interleave(\n      lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n  dataset = dataset.map(\n      lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n          x, data_size, sample_size, num_in_channels, clip_and_normalize,\n          clip_and_rescale, random_crop, center_crop),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  dataset = dataset.batch(batch_size)\n  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n  return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:47.414004Z","iopub.execute_input":"2025-05-08T03:42:47.414210Z","iopub.status.idle":"2025-05-08T03:42:47.434671Z","shell.execute_reply.started":"2025-05-08T03:42:47.414189Z","shell.execute_reply":"2025-05-08T03:42:47.434024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 32\nSAMPLE_SIZE = 64\n\ntrain_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_train*', \n    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n    clip_and_rescale=False, random_crop=True, center_crop=False)\n\nvalidation_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_eval*', \n    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n    clip_and_rescale=False, random_crop=True, center_crop=False)\n\ntest_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_test*',\n    data_size=64, sample_size=SAMPLE_SIZE, batch_size=BATCH_SIZE,\n    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n    clip_and_rescale=False, random_crop=True, center_crop=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:47.435274Z","iopub.execute_input":"2025-05-08T03:42:47.435483Z","iopub.status.idle":"2025-05-08T03:42:48.608478Z","shell.execute_reply.started":"2025-05-08T03:42:47.435441Z","shell.execute_reply":"2025-05-08T03:42:48.607853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for x, y in train_dataset.take(1):\n    print(x.shape, y.shape)\n\nfor x, y in validation_dataset.take(1):\n    print(x.shape, y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:48.609287Z","iopub.execute_input":"2025-05-08T03:42:48.609525Z","iopub.status.idle":"2025-05-08T03:42:48.803212Z","shell.execute_reply.started":"2025-05-08T03:42:48.609507Z","shell.execute_reply":"2025-05-08T03:42:48.802437Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# convert to torch ( i dont know how to tensor flow)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport torch\nfrom torch.utils.data import Dataset\n\n\n\nclass TFToTorchDataset(Dataset):\n    def __init__(self, tf_dataset, clean=False):\n        self.samples = []\n        for x, y in tf_dataset.as_numpy_iterator():\n            for i in range(x.shape[0]):\n                # Convert x: (32, 32, 12) → (12, 32, 32)\n                x_i = tf.transpose(x[i], perm=[2, 0, 1]).numpy()\n                # Convert y: (32, 32, 1) → (1, 32, 32)\n                y_i = tf.transpose(y[i], perm=[2, 0, 1]).numpy()\n                \n                if clean:\n                    if (y_i == -1).any():\n                        continue  # Skip this sample\n                self.samples.append((\n                    torch.tensor(x_i, dtype=torch.float32),\n                    torch.tensor(y_i, dtype=torch.float32)\n                ))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:48.804485Z","iopub.execute_input":"2025-05-08T03:42:48.804682Z","iopub.status.idle":"2025-05-08T03:42:48.810853Z","shell.execute_reply.started":"2025-05-08T03:42:48.804667Z","shell.execute_reply":"2025-05-08T03:42:48.810043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch_dataset = TFToTorchDataset(train_dataset, clean=True)\ntrain_loader = torch.utils.data.DataLoader(torch_dataset, batch_size=32, shuffle=True)\n\ntorch_dataset_val =  TFToTorchDataset(validation_dataset, clean=True)\nval_loader = torch.utils.data.DataLoader(torch_dataset_val, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:42:48.813076Z","iopub.execute_input":"2025-05-08T03:42:48.813267Z","iopub.status.idle":"2025-05-08T03:43:21.087737Z","shell.execute_reply.started":"2025-05-08T03:42:48.813253Z","shell.execute_reply":"2025-05-08T03:43:21.087110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N = 5   \ndataiter = iter(train_loader)\n\nimage_list = []\nlabel_list = []\n#assume batch size equal to 1, otherwise divide N by batch size\nfor i in range(0, N): \n  image, label = next(dataiter)\n  image_list.append(image)\n  label_list.append(label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:43:21.091267Z","iopub.execute_input":"2025-05-08T03:43:21.091532Z","iopub.status.idle":"2025-05-08T03:43:21.166586Z","shell.execute_reply.started":"2025-05-08T03:43:21.091514Z","shell.execute_reply":"2025-05-08T03:43:21.165801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(label_list[0][0][0][0]))\n# print(label_list[0][0])\nplt.imshow(label_list[4][0][0], cmap='viridis', interpolation='nearest')\nplt.colorbar()  \nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:43:21.167460Z","iopub.execute_input":"2025-05-08T03:43:21.167691Z","iopub.status.idle":"2025-05-08T03:43:21.350637Z","shell.execute_reply.started":"2025-05-08T03:43:21.167674Z","shell.execute_reply":"2025-05-08T03:43:21.349869Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# fully conv model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=12, out_channels=1):\n        super(UNet, self).__init__()\n\n        def conv_block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_c),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_c),\n                nn.ReLU(inplace=True),\n            )\n\n        self.encoder1 = conv_block(in_channels, 64)\n        self.encoder2 = conv_block(64, 128)\n        self.encoder3 = conv_block(128, 256)\n\n        self.pool = nn.MaxPool2d(2)\n\n        self.bottleneck = conv_block(256, 512)\n\n        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.decoder3 = conv_block(512, 256)\n\n        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.decoder2 = conv_block(256, 128)\n\n        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.decoder1 = conv_block(128, 64)\n\n        self.output_layer = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        # Encoder\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(self.pool(e1))\n        e3 = self.encoder3(self.pool(e2))\n\n        # Bottleneck\n        b = self.bottleneck(self.pool(e3))\n\n        # Decoder\n        d3 = self.up3(b)\n        d3 = self.decoder3(torch.cat([d3, e3], dim=1))\n\n        d2 = self.up2(d3)\n        d2 = self.decoder2(torch.cat([d2, e2], dim=1))\n\n        d1 = self.up1(d2)\n        d1 = self.decoder1(torch.cat([d1, e1], dim=1))\n\n        # return torch.sigmoid(self.output_layer(d1))\n        return self.output_layer(d1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:43:21.351533Z","iopub.execute_input":"2025-05-08T03:43:21.351988Z","iopub.status.idle":"2025-05-08T03:43:21.360260Z","shell.execute_reply.started":"2025-05-08T03:43:21.351962Z","shell.execute_reply":"2025-05-08T03:43:21.359497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef show_prediction(model, val_loader):\n    model.eval()\n    with torch.no_grad():\n        x_val, y_val = next(iter(val_loader))\n        x_val, y_val = x_val.to(device), y_val.to(device)\n        y_val = (y_val == 1).float()\n\n        pred = model(x_val)\n        pred_bin = (pred > 0.5).float()\n\n        # Show first sample\n        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n        axs[0].imshow(x_val[0, 11].cpu(), cmap='gray')\n        axs[0].set_title(\"Previous Fire Mask\")\n        axs[1].imshow(y_val[0, 0].cpu(), cmap='gray')\n        axs[1].set_title(\"Ground Truth\")\n        axs[2].imshow(pred_bin[0, 0].cpu(), cmap='gray')\n        axs[2].set_title(\"Prediction\")\n        for ax in axs:\n            ax.axis('off')\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:43:21.360924Z","iopub.execute_input":"2025-05-08T03:43:21.361101Z","iopub.status.idle":"2025-05-08T03:43:21.378996Z","shell.execute_reply.started":"2025-05-08T03:43:21.361079Z","shell.execute_reply":"2025-05-08T03:43:21.378268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_iou(pred, target, threshold=0.5, eps=1e-6):\n    pred_bin = (pred > threshold).float()\n    target_bin = (target > 0.5).float()\n\n    intersection = (pred_bin * target_bin).sum(dim=(1, 2, 3))\n    union = (pred_bin + target_bin - pred_bin * target_bin).sum(dim=(1, 2, 3))\n    iou = (intersection + eps) / (union + eps)\n    return iou.mean().item()\n\ndef compute_accuracy(pred, target, threshold=0.5):\n    pred_bin = (pred > threshold).float()\n    correct = (pred_bin == target).float()\n    return correct.mean().item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:43:21.379776Z","iopub.execute_input":"2025-05-08T03:43:21.380040Z","iopub.status.idle":"2025-05-08T03:43:21.393922Z","shell.execute_reply.started":"2025-05-08T03:43:21.380014Z","shell.execute_reply":"2025-05-08T03:43:21.393172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = UNet().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# criterion = nn.BCELoss()\n# use instead bc we have class imbalance\nweight_value = 0.85/0.15 # TODO pull the real percent background, percent fire from the dataset, i just guess 15% fire\npos_weight = torch.tensor([weight_value], device=device) \ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n\n        pred = model(x)\n        loss = criterion(pred, y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * x.size(0)\n\n    train_loss /= len(train_loader.dataset)\n\n    # === Validation ===\n    model.eval()\n    val_loss = 0.0\n    total_iou = 0.0\n    total_acc = 0.0\n    n_samples = 0\n    \n    with torch.no_grad():\n        for x_val, y_val in val_loader:\n            x_val, y_val = x_val.to(device), y_val.to(device)\n            y_val = (y_val == 1).float()\n    \n            pred_val = torch.sigmoid(model(x_val))\n            loss = criterion(pred_val, y_val)\n            val_loss += loss.item() * x_val.size(0)\n    \n            # Metrics\n            batch_iou = compute_iou(pred_val, y_val)\n            batch_acc = compute_accuracy(pred_val, y_val)\n    \n            total_iou += batch_iou * x_val.size(0)\n            total_acc += batch_acc * x_val.size(0)\n            n_samples += x_val.size(0)\n    \n    val_loss /= len(val_loader.dataset)\n    mean_iou = total_iou / n_samples\n    mean_acc = total_acc / n_samples\n\n\n    print(f\"Epoch {epoch+1:2d} | Train Loss: {train_loss:.4f} | \"\n      f\"Val Loss: {val_loss:.4f} | IoU: {mean_iou:.4f} | Acc: {mean_acc:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:43:21.394662Z","iopub.execute_input":"2025-05-08T03:43:21.394843Z","iopub.status.idle":"2025-05-08T03:48:19.091634Z","shell.execute_reply.started":"2025-05-08T03:43:21.394830Z","shell.execute_reply":"2025-05-08T03:48:19.090969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_prediction(model, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T03:48:39.739283Z","iopub.execute_input":"2025-05-08T03:48:39.739987Z","iopub.status.idle":"2025-05-08T03:48:39.924659Z","shell.execute_reply.started":"2025-05-08T03:48:39.739966Z","shell.execute_reply":"2025-05-08T03:48:39.923909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# vision transformer (IGNORE)","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torchvision.models.vision_transformer import VisionTransformer\n\n# class FireMaskViT(nn.Module):\n#     def __init__(self, input_channels, image_size=64, patch_size=8, num_classes=1, hidden_dim=768, num_layers=12, num_heads=12):\n#         super().__init__()\n\n#         assert image_size % patch_size == 0, \"image_size must be divisible by patch_size\"\n\n#         num_patches = (image_size // patch_size) ** 2\n        \n#         # VisionTransformer expects 3 input channels → we re-define input projection\n#         self.vit = VisionTransformer(\n#             image_size=image_size,\n#             patch_size=patch_size,\n#             num_layers=num_layers,\n#             num_heads=num_heads,\n#             hidden_dim=hidden_dim,\n#             mlp_dim=hidden_dim * 4,\n#             num_classes=0,  # no classification head\n#         )\n\n#         # Replace input projection layer to accept input_channels instead of 3\n#         self.vit.conv_proj = nn.Conv2d(input_channels, hidden_dim, kernel_size=patch_size, stride=patch_size)\n\n#         # self.vit.encoder.pos_embedding = nn.Parameter(torch.randn(1, num_patches, hidden_dim))\n#         self.vit.encoder.cls_token = None\n#         self.vit.encoder.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, hidden_dim))\n\n\n\n#         # Output: project transformer embedding back to patch predictions\n#         # num_patches = (image_size // patch_size) ** 2\n#         self.output_head = nn.Linear(hidden_dim, patch_size * patch_size)\n\n#     def forward(self, x):\n#         # x: (batch_size, C, 64, 64)\n#         embeddings = self.vit(x)  # (batch_size, num_patches, hidden_dim)\n#         out = self.output_head(embeddings)  # (batch_size, num_patches, patch_pixels)\n#         # Reshape output back to (batch_size, 1, 64, 64)\n#         B, N, P = out.shape\n#         patch_size = int(P ** 0.5)\n#         H = W = int(N ** 0.5)\n#         out = out.view(B, H, W, patch_size, patch_size).permute(0,1,3,2,4).reshape(B, 1, H*patch_size, W*patch_size)\n#         return out\n\n# # Example usage\n# input_channels = 12  # prev fire mask + NDVI + temp + humidity + ...\n# model = FireMaskViT(input_channels=input_channels)\n\n# dummy_input = torch.randn(2, input_channels, 64, 64)\n# output = model(dummy_input)  # → output shape: (2, 1, 64, 64)\n# print(output.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:16:47.645181Z","iopub.execute_input":"2025-05-08T04:16:47.645740Z","iopub.status.idle":"2025-05-08T04:16:48.909794Z","shell.execute_reply.started":"2025-05-08T04:16:47.645718Z","shell.execute_reply":"2025-05-08T04:16:48.908763Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torchvision.models.vision_transformer import VisionTransformer\n\n# class FireMaskViT(nn.Module):\n#     def __init__(self, input_channels=12, image_size=64, patch_size=8, hidden_dim=768, num_classes=1):\n#         super().__init__()\n\n#         assert image_size % patch_size == 0, \"image_size must be divisible by patch_size\"\n#         num_patches = (image_size // patch_size) ** 2\n\n#         # 1. Load pretrained ViT or define from scratch\n#         self.vit = VisionTransformer(\n#             image_size=image_size,\n#             patch_size=patch_size,\n#             num_layers=12,\n#             num_heads=12,\n#             hidden_dim=hidden_dim,\n#             mlp_dim=hidden_dim * 4,\n#             num_classes=0,  # No classifier head\n#         )\n\n#         # 2. Replace input stem to accept 12 channels\n#         self.vit.conv_proj = nn.Conv2d(input_channels, hidden_dim, kernel_size=patch_size, stride=patch_size)\n\n#         # 3. Replace positional embedding with correct size\n#         self.vit.encoder.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, hidden_dim))\n\n#         # 4. Decoder head to produce patch_size x patch_size pixels per patch\n#         self.output_head = nn.Linear(hidden_dim, patch_size * patch_size)\n\n#         self.patch_size = patch_size\n#         self.image_size = image_size\n\n#     def forward(self, x):\n#         B = x.size(0)\n\n#         # x: (B, 12, 64, 64)\n#         x = self.vit(x)  # (B, num_patches, hidden_dim)\n#         x = self.output_head(x)  # (B, num_patches, patch_area)\n\n#         P = self.patch_size\n#         N = self.image_size // P\n#         out = x.view(B, N, N, P, P).permute(0, 1, 3, 2, 4).reshape(B, 1, self.image_size, self.image_size)\n#         return out\n\n\n# model = FireMaskViT(input_channels=12, image_size=64, patch_size=8)\n# dummy_input = torch.randn(2, 12, 64, 64)\n# out = model(dummy_input)\n# print(out.shape)  # Expected: torch.Size([2, 1, 64, 64])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:15:25.086572Z","iopub.execute_input":"2025-05-08T04:15:25.087266Z","iopub.status.idle":"2025-05-08T04:15:26.227469Z","shell.execute_reply.started":"2025-05-08T04:15:25.087241Z","shell.execute_reply":"2025-05-08T04:15:26.226456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# timm vt","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\n\nclass FireMaskViT(nn.Module):\n    def __init__(self, input_channels=12, image_size=64, patch_size=8, hidden_dim=768):\n        super().__init__()\n\n        assert image_size % patch_size == 0, \"Image size must be divisible by patch size.\"\n\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.hidden_dim = hidden_dim\n        self.num_patches = (image_size // patch_size) ** 2\n\n        self.vit = timm.create_model(\n            'vit_base_patch16_224',\n            pretrained=False,\n            img_size=image_size,\n            patch_size=patch_size,\n            in_chans=input_channels,\n            num_classes=0  # no classifier head\n        )\n\n        # print(type(self.vit))\n\n\n        self.output_head = nn.Linear(self.vit.embed_dim, patch_size * patch_size)\n\n    def forward(self, x):\n        B = x.shape[0]\n\n        x = self.vit.forward_features(x)\n        x = x[:, 1:, :]    \n        # print(\"vit output:\", x.shape)\n\n        x = self.output_head(x)  # shape (B, num_patches, patch_area)\n\n        P = self.patch_size\n        N = self.image_size // P  # patches along each dimension\n\n        x = x.view(B, N, N, P, P).permute(0, 1, 3, 2, 4).reshape(B, 1, self.image_size, self.image_size)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:39:51.562916Z","iopub.execute_input":"2025-05-08T04:39:51.563178Z","iopub.status.idle":"2025-05-08T04:39:51.570047Z","shell.execute_reply.started":"2025-05-08T04:39:51.563159Z","shell.execute_reply":"2025-05-08T04:39:51.569321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = FireMaskViT(input_channels=12, image_size=64, patch_size=8)\n# dummy_input = torch.randn(32, 12, 64, 64)\n# output = model(dummy_input)\n# print(output.shape)  # Should be: torch.Size([32, 1, 64, 64])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:38:15.841758Z","iopub.execute_input":"2025-05-08T04:38:15.842036Z","iopub.status.idle":"2025-05-08T04:38:19.231225Z","shell.execute_reply.started":"2025-05-08T04:38:15.842015Z","shell.execute_reply":"2025-05-08T04:38:19.230502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = FireMaskViT(input_channels=12).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\npos_weight = torch.tensor([10.0]).to(device)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n# criterion = nn.BCEWithLogitsLoss()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:51:36.379105Z","iopub.execute_input":"2025-05-08T04:51:36.379663Z","iopub.status.idle":"2025-05-08T04:51:37.626162Z","shell.execute_reply.started":"2025-05-08T04:51:36.379642Z","shell.execute_reply":"2025-05-08T04:51:37.625581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(1, 5):\n    model.train()\n    train_loss = 0\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    \n    val_loss = 0\n    model.eval()\n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            val_loss += loss.item()\n\n    print(f\"Epoch {epoch:2d} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:51:39.502542Z","iopub.execute_input":"2025-05-08T04:51:39.503161Z","iopub.status.idle":"2025-05-08T04:59:41.420417Z","shell.execute_reply.started":"2025-05-08T04:51:39.503141Z","shell.execute_reply":"2025-05-08T04:59:41.419578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(model, dataloader, threshold=0.5):\n    model.eval()\n    correct, total = 0, 0\n    intersection, union = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = torch.sigmoid(model(X)) > threshold\n            correct += (pred == y).sum().item()\n            total += torch.numel(y)\n            intersection += (pred & (y > 0.5)).sum().item()\n            union += ((pred | (y > 0.5))).sum().item()\n    acc = correct / total\n    iou = intersection / union if union else 0\n    print(f\"Pixel Accuracy: {acc:.4f}, IoU: {iou:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T05:00:21.535688Z","iopub.execute_input":"2025-05-08T05:00:21.536360Z","iopub.status.idle":"2025-05-08T05:00:21.541640Z","shell.execute_reply.started":"2025-05-08T05:00:21.536340Z","shell.execute_reply":"2025-05-08T05:00:21.540837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_prediction(model, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T04:59:52.117806Z","iopub.execute_input":"2025-05-08T04:59:52.118467Z","iopub.status.idle":"2025-05-08T04:59:53.030251Z","shell.execute_reply.started":"2025-05-08T04:59:52.118429Z","shell.execute_reply":"2025-05-08T04:59:53.029593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate(model, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T05:00:34.208675Z","iopub.execute_input":"2025-05-08T05:00:34.209196Z","iopub.status.idle":"2025-05-08T05:00:39.030196Z","shell.execute_reply.started":"2025-05-08T05:00:34.209172Z","shell.execute_reply":"2025-05-08T05:00:39.029428Z"}},"outputs":[],"execution_count":null}]}